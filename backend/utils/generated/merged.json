[
    {
        "key": "merged_topic_0_0",
        "label": "conceptual, classes, class, learning, unlik /  classiﬁcation, automatically, classes, taxonomy, people",
        "input": "<task:merge> <sos>\nCluster Analysis Imagine jects analysis where unlik classi cation class Clustering grouping classes clusters jects within cluster similarit comparison another dissimilar jects other clusters\ngrouping ysical abstract jects classes similar jects called clustering\nUnlik classi cation clustering unsup ervised learning prede ned classes classlab training examples\nreason clustering learning observ ation rather arning examples\nconceptual clustering group jects forms class describable concept\nConceptual clustering consists disco appropriate classes forms descriptions class classi cation\ngeneral clustering metho classi ed follo categories\nddimensional jects cluster cluster de ned where cluster linear square clustering feature essen tially summary statistics cluster zeroth second momen cluster statistical view\nStatistical approac Conceptual clusterin clustering learning unlab jects duces classi cation jects\nUnlik tional clustering primarily ti es groups jects conceptual clustering further nding haracteristic descriptions group where group represen concept class\nmetho conceptual clustering adopt statistical approac probabilit measuremen determining concepts clusters\nCOBWEB opular simple metho incremen conceptual clustering input jects describ categorical attributev pairs\nImpro inference through conceptual clustering\n\n++++\n\nClustering for Understanding Classes or conceptually meaningful groups of objects that share common characteristics play an important role in how people analyze and describe the world\nIndeed human beings are skilled at dividing objects into groups ( clustering ) and assigning particular objects to these groups ( classiﬁcation )\nFor example even relatively young children can quickly label the objects in a photograph as buildings vehicles people ani mals plants etc\nIn the context of understanding data clusters are potential classes and cluster analysis is the study of techniques for automatically ﬁnding classes\nBiologists have spent many years creating a taxonomy ( hi erarchical classiﬁcation ) of all living things kingdom phylum class order family genus and species\nThus it is perhaps not surprising that much of the early work in cluster analysis sought to create a discipline of mathematical taxonomy that could automatically ﬁnd such classiﬁ cation structures\nFor instance clustering can be regarded as a form of classiﬁcation in that it creates a labeling of objects with class ( cluster ) labels\nFor this reason cluster analysis is sometimes referred to as unsupervised classiﬁcation\n<eos>"
    },
    {
        "key": "merged_topic_0_1",
        "label": "conceptual, classes, class, learning, unlik /  classiﬁcation, automatically, classes, taxonomy, people",
        "input": "<task:merge> <sos>\nLearning observ ation Conceptual clustering\n\n++++\n\nWhen the term classiﬁcation is used without any qualiﬁcation within data mining it typically refers to supervised classiﬁcation\nMotivations for such an analysis are the comparison of clustering techniques with the ground truth or the evaluation of the extent to which a manual classiﬁcation process can be automatically produced by cluster analysis\n<eos>"
    },
    {
        "key": "merged_topic_1_0",
        "label": "conceptual, classes, class, learning, unlik /  analysis, groups, concepts, divides, exploratory",
        "input": "<task:merge> <sos>\nCluster Analysis Imagine jects analysis where unlik classi cation class Clustering grouping classes clusters jects within cluster similarit comparison another dissimilar jects other clusters\ngrouping ysical abstract jects classes similar jects called clustering\nUnlik classi cation clustering unsup ervised learning prede ned classes classlab training examples\nreason clustering learning observ ation rather arning examples\nconceptual clustering group jects forms class describable concept\nConceptual clustering consists disco appropriate classes forms descriptions class classi cation\ngeneral clustering metho classi ed follo categories\nddimensional jects cluster cluster de ned where cluster linear square clustering feature essen tially summary statistics cluster zeroth second momen cluster statistical view\nStatistical approac Conceptual clusterin clustering learning unlab jects duces classi cation jects\nUnlik tional clustering primarily ti es groups jects conceptual clustering further nding haracteristic descriptions group where group represen concept class\nmetho conceptual clustering adopt statistical approac probabilit measuremen determining concepts clusters\nCOBWEB opular simple metho incremen conceptual clustering input jects describ categorical attributev pairs\nImpro inference through conceptual clustering\nLearning observ ation Conceptual clustering\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms Cluster analysis divides data into groups ( clusters ) that are meaningful useful or both\nThe following are some examples Cluster Analysis Basic Concepts and Algorithms Biology\nWhat Is Cluster Analysis\nCluster analysis groups data objects based only on information found in the data that describes the objects and their relationships\nCluster analysis is related to other techniques that are used to divide data objects into groups\nRoad Map to introduce many of the concepts involved in cluster analysis\nMany times cluster analysis is conducted as a part of an exploratory data analysis\n<eos>"
    },
    {
        "key": "merged_topic_2_0",
        "label": "means, partitions, partitioning, cluster, algorithm /  greater, types, group, aims, highlevel",
        "input": "<task:merge> <sos>\nClustering jects based means metho cases function medoids clustering\nartition metho database jects tuples partitioning metho constructs partitions data where partition represen cluster classi es groups together satisfy follo requiremen group least ject elong exactly group\nNotice second requiremen relaxed fuzzy partitioning hniques\npartitions construct partitioning metho creates initial partitioning\ngeneral criterion partitioning jects cluster close related other whereas jects di eren clusters apart di eren There arious kinds other criteria judging qualit partitions\nglobal optimalit partitioningbased clustering require exhaustiv umeration ossible partitions\nInstead applications adopt opular heuristic metho algorithm where cluster represen jects cluster doids algorithm where cluster represen jects cated cluster\nAlgorithm means ) means algorithm partitioning based jects cluster\nMetho arbitrarily jects initial cluster ters ( re ) assign cluster similar based jects cluster cluster means calculate jects cluster hange means algorithm\nRecall Equation transform dissimilarit similarit ecien Classical partitioning metho means medoids ellkno commonl partitioning metho doids their ariations\ntroidbased hnique means metho means algorithm input parameter partitions jects clusters resulting tracluster similarit whereas tercluster similarit Cluster similarit measured regard jects cluster clusters enter avity\nmeans cedure summarized algorithm attempts determine partitions minimi squarederror function\nThere quite arian means metho These di er selection initial means calculation dissimilarit strategies calculating cluster means\nAnother arian means metho extends means paradigm cluster categorical replacing means clusters using dissimilarit measures categorical jects using frequencybased metho clusters\nmeans metho tegrated cluster mixed umeric categorical alues resulting protot metho ectation Maximization ) algorithm extends means paradigm di eren Instead assigning dedicated cluster assigns cluster according represen probabilit ership\nattempts determine partitions jects\nAfter built clustering algorithm ypical partitioning algorithm Phase tries clusters ailable resources\n\n++++\n\nWe provide some speciﬁc examples organized by whether the purpose of the clustering is understanding or utility\nThis idea is made a highlevel overview of clustering including a discussion of the various ap proaches to dividing objects into sets of clusters and the diﬀerent types of clusters\nFirst we further deﬁne cluster analysis illustrating why it is diﬃcult and explaining its relationship to other techniques that group data\n<eos>"
    },
    {
        "key": "merged_topic_2_1",
        "label": "means, partitions, partitioning, cluster, algorithm /  greater, types, group, aims, highlevel",
        "input": "<task:merge> <sos>\nartition sample partitions artially cluster partitions clusters Clustering jects ) CURE\njects partitioned partially clustered\ncluster jects clusters therefore First jects sampled These jects distributed partitions taining partially cluster partitions clusters based minim distance\n\n++++\n\nThen we explore two important topics ( ) diﬀerent ways to group a set of objects into a set of clusters and ( ) types of clusters\nThe greater the similarity ( or homogeneity ) within a group and the greater the diﬀerence between groups the better or more distinct the clustering\nNonetheless some work in graph partitioning and in image and market segmentation is related to cluster analysis\nDiﬀerent Types of Clusters Clustering aims to ﬁnd useful groups of objects ( clusters ) where usefulness is deﬁned by the goals of the data analysis\n<eos>"
    },
    {
        "key": "merged_topic_3_0",
        "label": "means, partitions, partitioning, cluster, algorithm /  versus, segmentation, division, partitional, partitioning",
        "input": "<task:merge> <sos>\nClustering jects based means metho cases function medoids clustering\nartition metho database jects tuples partitioning metho constructs partitions data where partition represen cluster classi es groups together satisfy follo requiremen group least ject elong exactly group\nNotice second requiremen relaxed fuzzy partitioning hniques\npartitions construct partitioning metho creates initial partitioning\ngeneral criterion partitioning jects cluster close related other whereas jects di eren clusters apart di eren There arious kinds other criteria judging qualit partitions\nglobal optimalit partitioningbased clustering require exhaustiv umeration ossible partitions\nInstead applications adopt opular heuristic metho algorithm where cluster represen jects cluster doids algorithm where cluster represen jects cated cluster\nAlgorithm means ) means algorithm partitioning based jects cluster\nMetho arbitrarily jects initial cluster ters ( re ) assign cluster similar based jects cluster cluster means calculate jects cluster hange means algorithm\nRecall Equation transform dissimilarit similarit ecien Classical partitioning metho means medoids ellkno commonl partitioning metho doids their ariations\ntroidbased hnique means metho means algorithm input parameter partitions jects clusters resulting tracluster similarit whereas tercluster similarit Cluster similarit measured regard jects cluster clusters enter avity\nmeans cedure summarized algorithm attempts determine partitions minimi squarederror function\nThere quite arian means metho These di er selection initial means calculation dissimilarit strategies calculating cluster means\nAnother arian means metho extends means paradigm cluster categorical replacing means clusters using dissimilarit measures categorical jects using frequencybased metho clusters\nmeans metho tegrated cluster mixed umeric categorical alues resulting protot metho ectation Maximization ) algorithm extends means paradigm di eren Instead assigning dedicated cluster assigns cluster according represen probabilit ership\nattempts determine partitions jects\nAfter built clustering algorithm ypical partitioning algorithm Phase tries clusters ailable resources\nartition sample partitions artially cluster partitions clusters Clustering jects ) CURE\njects partitioned partially clustered\n\n++++\n\nAlso while the terms segmentation and partitioning are sometimes used as synonyms for clustering these terms are frequently used for approaches outside the traditional bounds of cluster analysis\nFor example the term partitioning is often used in connection with techniques that divide graphs into subgraphs and that are not strongly connected to clustering\n<eos>"
    },
    {
        "key": "merged_topic_3_1",
        "label": "means, partitions, partitioning, cluster, algorithm /  versus, segmentation, division, partitional, partitioning",
        "input": "<task:merge> <sos>\ncluster jects clusters therefore First jects sampled These jects distributed partitions taining partially cluster partitions clusters based minim distance\n\n++++\n\nSegmentation often refers to the division of data into groups using simple techniques eg an image can be split into segments based only on pixel intensity and color or people can be divided into groups based on their income\nDiﬀerent Types of Clusterings An entire collection of clusters is commonly referred to as a clustering and in this section we distinguish various types of clusterings hierarchical ( nested ) versus partitional ( unnested ) exclusive versus overlapping versus fuzzy and complete versus partial\nA partitional clustering is simply a division of the set of data objects into nonoverlapping subsets ( clusters ) such that each data object is in exactly one a partitional clustering\nMore generally such algorithms are typi cally used because the underlying application eg creation of a taxonomy requires a hierarchy\n<eos>"
    },
    {
        "key": "merged_topic_4_0",
        "label": "means, partitions, partitioning, cluster, algorithm /  analysis, groups, concepts, divides, exploratory",
        "input": "<task:merge> <sos>\nClustering jects based means metho cases function medoids clustering\nartition metho database jects tuples partitioning metho constructs partitions data where partition represen cluster classi es groups together satisfy follo requiremen group least ject elong exactly group\nNotice second requiremen relaxed fuzzy partitioning hniques\npartitions construct partitioning metho creates initial partitioning\ngeneral criterion partitioning jects cluster close related other whereas jects di eren clusters apart di eren There arious kinds other criteria judging qualit partitions\nglobal optimalit partitioningbased clustering require exhaustiv umeration ossible partitions\nInstead applications adopt opular heuristic metho algorithm where cluster represen jects cluster doids algorithm where cluster represen jects cated cluster\nAlgorithm means ) means algorithm partitioning based jects cluster\nMetho arbitrarily jects initial cluster ters ( re ) assign cluster similar based jects cluster cluster means calculate jects cluster hange means algorithm\nRecall Equation transform dissimilarit similarit ecien Classical partitioning metho means medoids ellkno commonl partitioning metho doids their ariations\ntroidbased hnique means metho means algorithm input parameter partitions jects clusters resulting tracluster similarit whereas tercluster similarit Cluster similarit measured regard jects cluster clusters enter avity\nmeans cedure summarized algorithm attempts determine partitions minimi squarederror function\nThere quite arian means metho These di er selection initial means calculation dissimilarit strategies calculating cluster means\nAnother arian means metho extends means paradigm cluster categorical replacing means clusters using dissimilarit measures categorical jects using frequencybased metho clusters\nmeans metho tegrated cluster mixed umeric categorical alues resulting protot metho ectation Maximization ) algorithm extends means paradigm di eren Instead assigning dedicated cluster assigns cluster according represen probabilit ership\nattempts determine partitions jects\nAfter built clustering algorithm ypical partitioning algorithm Phase tries clusters ailable resources\nartition sample partitions artially cluster partitions clusters Clustering jects ) CURE\njects partitioned partially clustered\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms Cluster analysis divides data into groups ( clusters ) that are meaningful useful or both\nThe following are some examples Cluster Analysis Basic Concepts and Algorithms Biology\nWhat Is Cluster Analysis\nCluster analysis groups data objects based only on information found in the data that describes the objects and their relationships\n<eos>"
    },
    {
        "key": "merged_topic_4_1",
        "label": "means, partitions, partitioning, cluster, algorithm /  analysis, groups, concepts, divides, exploratory",
        "input": "<task:merge> <sos>\ncluster jects clusters therefore First jects sampled These jects distributed partitions taining partially cluster partitions clusters based minim distance\n\n++++\n\nCluster analysis is related to other techniques that are used to divide data objects into groups\nRoad Map to introduce many of the concepts involved in cluster analysis\nMany times cluster analysis is conducted as a part of an exploratory data analysis\n<eos>"
    },
    {
        "key": "merged_topic_5_0",
        "label": "structures, structure, criterion, erformed, tries /  concepts, algorithms, basic, analysis, wellseparated",
        "input": "<task:merge> <sos>\nWhat typic applic ations clustering\nclustered tains jects represen ersons houses cumen tries memorybased clustering algorithms ypically erate either follo structures\nclustering erations erformed structure tized space )\nfollo sections examine clustering metho detail\nypically squarederror criterion used de ned where squareerror jects database space represen ject cluster ultidim ensional criterion tries resulting clusters compact separate ossible\nstructure clustering summarize jects discarded compressed\nerformance CLARANS further impro exploring spatial structures Rtrees cusing hniques\ne ectiv incremen dynamic clustering incoming jects\nLets closer emen tioned structures\nconstruct di eren clusterings ultaneously jects should cessed eci c order\nexample habilit simple dimensional presen general erview structured clustered\nularit cessing increase substan tially ottom structure coarse reduce qualit cluster analysis\neCluster Clustering using transformation eCluster ultiresolution clustering algorithm summarizes osing ultidim sional structure space\nFirst vides unsup ervised clustering\nHence conceptual clustering ostep cess clustering erformed follo haracterization\nurthermore classi cation heigh balanced input data cause space complexit degrade dramatically CLASSIT extension COBWEB incremen clustering alued ) data\nclustering constrained factors olving cation bridges a ect accessibilit Additional constrain limitati district forming region\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms ( a ) Wellseparated clusters\nWe illustrate this with the set of two Cluster Analysis Basic Concepts and Algorithms ( a ) Optimal clustering\n( b ) Suboptimal clustering\nIn this way we obtain a set of initial Cluster Analysis Basic Concepts and Algorithms ( a ) Initial points\nCluster Analysis Basic Concepts and Algorithms ( a ) Iteration\nCluster Analysis Basic Concepts and Algorithms ( a ) Unequal sizes\nCluster Analysis Basic Concepts and Algorithms ( a ) Single link clustering\nCluster Analysis Basic Concepts and Algorithms ( a ) Original points\n<eos>"
    },
    {
        "key": "merged_topic_6_0",
        "label": "ctivity, interc, graph, ternal, static /  concepts, algorithms, basic, analysis, wellseparated",
        "input": "<task:merge> <sos>\nmeasures similarit clusters comparing interc ctivity clusters against usersp eci ed static interc ctivity where terconn clusters de ned links clusters common Data Set the Graph Partition Partitions Merge Final Clusters a Sparse Graph Construct k nearest Neighbor Graph CHAMELEON Hierarc hical clustering based knearest neigh dynamic deling\nmerge based dynamic facilitates disco natural homogeneous clusters applies similarit function eci ed\ndetermine pairs similar clusters accoun terconnectivit closeness clusters ecially ternal haracteristics clusters themselv static usersupplied automatically adapt ternal haracteristics clusters merged\nCHAMELEON determines similarit clusters according their elative interc ctivity their elative closeness relativ terconne ctivi clusters de ned absolute connectivit normalized ternal terconnectivit clusters where dgecut cluster taining cluster similarly mincut edges partition graph roughly equal parts )\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms ( a ) Wellseparated clusters\nWe illustrate this with the set of two Cluster Analysis Basic Concepts and Algorithms ( a ) Optimal clustering\n( b ) Suboptimal clustering\nIn this way we obtain a set of initial Cluster Analysis Basic Concepts and Algorithms ( a ) Initial points\nCluster Analysis Basic Concepts and Algorithms ( a ) Iteration\nCluster Analysis Basic Concepts and Algorithms ( a ) Unequal sizes\nCluster Analysis Basic Concepts and Algorithms ( a ) Single link clustering\nCluster Analysis Basic Concepts and Algorithms ( a ) Original points\n<eos>"
    },
    {
        "key": "merged_topic_7_0",
        "label": "means, partitions, partitioning, cluster, algorithm /  concepts, algorithms, basic, analysis, wellseparated",
        "input": "<task:merge> <sos>\nClustering jects based means metho cases function medoids clustering\nartition metho database jects tuples partitioning metho constructs partitions data where partition represen cluster classi es groups together satisfy follo requiremen group least ject elong exactly group\nNotice second requiremen relaxed fuzzy partitioning hniques\npartitions construct partitioning metho creates initial partitioning\ngeneral criterion partitioning jects cluster close related other whereas jects di eren clusters apart di eren There arious kinds other criteria judging qualit partitions\nglobal optimalit partitioningbased clustering require exhaustiv umeration ossible partitions\nInstead applications adopt opular heuristic metho algorithm where cluster represen jects cluster doids algorithm where cluster represen jects cated cluster\nAlgorithm means ) means algorithm partitioning based jects cluster\nMetho arbitrarily jects initial cluster ters ( re ) assign cluster similar based jects cluster cluster means calculate jects cluster hange means algorithm\nRecall Equation transform dissimilarit similarit ecien Classical partitioning metho means medoids ellkno commonl partitioning metho doids their ariations\ntroidbased hnique means metho means algorithm input parameter partitions jects clusters resulting tracluster similarit whereas tercluster similarit Cluster similarit measured regard jects cluster clusters enter avity\nmeans cedure summarized algorithm attempts determine partitions minimi squarederror function\nThere quite arian means metho These di er selection initial means calculation dissimilarit strategies calculating cluster means\nAnother arian means metho extends means paradigm cluster categorical replacing means clusters using dissimilarit measures categorical jects using frequencybased metho clusters\nmeans metho tegrated cluster mixed umeric categorical alues resulting protot metho ectation Maximization ) algorithm extends means paradigm di eren Instead assigning dedicated cluster assigns cluster according represen probabilit ership\nattempts determine partitions jects\nAfter built clustering algorithm ypical partitioning algorithm Phase tries clusters ailable resources\nartition sample partitions artially cluster partitions clusters Clustering jects ) CURE\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms ( a ) Wellseparated clusters\nWe illustrate this with the set of two Cluster Analysis Basic Concepts and Algorithms ( a ) Optimal clustering\n( b ) Suboptimal clustering\nIn this way we obtain a set of initial Cluster Analysis Basic Concepts and Algorithms ( a ) Initial points\nCluster Analysis Basic Concepts and Algorithms ( a ) Iteration\n<eos>"
    },
    {
        "key": "merged_topic_7_1",
        "label": "means, partitions, partitioning, cluster, algorithm /  concepts, algorithms, basic, analysis, wellseparated",
        "input": "<task:merge> <sos>\njects partitioned partially clustered\ncluster jects clusters therefore First jects sampled These jects distributed partitions taining partially cluster partitions clusters based minim distance\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms ( a ) Unequal sizes\nCluster Analysis Basic Concepts and Algorithms ( a ) Single link clustering\nCluster Analysis Basic Concepts and Algorithms ( a ) Original points\n<eos>"
    },
    {
        "key": "merged_topic_8_0",
        "label": "ctivity, interc, graph, ternal, static /  cophenetic, correlation, popular, matrix, entries",
        "input": "<task:merge> <sos>\nmeasures similarit clusters comparing interc ctivity clusters against usersp eci ed static interc ctivity where terconn clusters de ned links clusters common Data Set the Graph Partition Partitions Merge Final Clusters a Sparse Graph Construct k nearest Neighbor Graph CHAMELEON Hierarc hical clustering based knearest neigh dynamic deling\nmerge based dynamic facilitates disco natural homogeneous clusters applies similarit function eci ed\ndetermine pairs similar clusters accoun terconnectivit closeness clusters ecially ternal haracteristics clusters themselv static usersupplied automatically adapt ternal haracteristics clusters merged\nCHAMELEON determines similarit clusters according their elative interc ctivity their elative closeness relativ terconne ctivi clusters de ned absolute connectivit normalized ternal terconnectivit clusters where dgecut cluster taining cluster similarly mincut edges partition graph roughly equal parts )\n\n++++\n\nSince these approaches are useful only for partitional sets of clusters we also describe the popular cophenetic correlation coeﬃcient which can be used for the unsupervised evaluation of a hierarchical clustering\nHere we discuss the cophenetic correlation a popular evaluation measure for hierarchical clusterings\nThe cophenetic distance between two objects is the proximity at which an agglomerative hierarchical clustering tech Cluster Evaluation Points Points Similarity ( a ) Similarity matrix sorted DBSCAN cluster labels\nIn a cophenetic distance matrix the entries are the cophenetic distances between each pair of objects\nThe cophenetic distance is diﬀerent for each hierarchical clustering of a set of points\ndata for this ﬁgure consists of the twodimensional points given in Table The CoPhenetic Correlation Coeﬃcient ( CPCC ) is the correlation between the entries of this matrix and the original dissimilarity matrix and is Cluster Analysis Basic Concepts and Algorithms a standard measure of how well a hierarchical clustering ( of a particular type ) ﬁts the data\n<eos>"
    },
    {
        "key": "merged_topic_9_0",
        "label": "ctivity, interc, graph, ternal, static /  greater, types, group, aims, highlevel",
        "input": "<task:merge> <sos>\nmeasures similarit clusters comparing interc ctivity clusters against usersp eci ed static interc ctivity where terconn clusters de ned links clusters common Data Set the Graph Partition Partitions Merge Final Clusters a Sparse Graph Construct k nearest Neighbor Graph CHAMELEON Hierarc hical clustering based knearest neigh dynamic deling\nmerge based dynamic facilitates disco natural homogeneous clusters applies similarit function eci ed\ndetermine pairs similar clusters accoun terconnectivit closeness clusters ecially ternal haracteristics clusters themselv static usersupplied automatically adapt ternal haracteristics clusters merged\nCHAMELEON determines similarit clusters according their elative interc ctivity their elative closeness relativ terconne ctivi clusters de ned absolute connectivit normalized ternal terconnectivit clusters where dgecut cluster taining cluster similarly mincut edges partition graph roughly equal parts )\n\n++++\n\nWe provide some speciﬁc examples organized by whether the purpose of the clustering is understanding or utility\nThis idea is made a highlevel overview of clustering including a discussion of the various ap proaches to dividing objects into sets of clusters and the diﬀerent types of clusters\nFirst we further deﬁne cluster analysis illustrating why it is diﬃcult and explaining its relationship to other techniques that group data\nThen we explore two important topics ( ) diﬀerent ways to group a set of objects into a set of clusters and ( ) types of clusters\nThe greater the similarity ( or homogeneity ) within a group and the greater the diﬀerence between groups the better or more distinct the clustering\nNonetheless some work in graph partitioning and in image and market segmentation is related to cluster analysis\nDiﬀerent Types of Clusters Clustering aims to ﬁnd useful groups of objects ( clusters ) where usefulness is deﬁned by the goals of the data analysis\n<eos>"
    },
    {
        "key": "merged_topic_10_0",
        "label": "conceptual, classes, class, learning, unlik /  greater, types, group, aims, highlevel",
        "input": "<task:merge> <sos>\nCluster Analysis Imagine jects analysis where unlik classi cation class Clustering grouping classes clusters jects within cluster similarit comparison another dissimilar jects other clusters\ngrouping ysical abstract jects classes similar jects called clustering\nUnlik classi cation clustering unsup ervised learning prede ned classes classlab training examples\nreason clustering learning observ ation rather arning examples\nconceptual clustering group jects forms class describable concept\nConceptual clustering consists disco appropriate classes forms descriptions class classi cation\ngeneral clustering metho classi ed follo categories\nddimensional jects cluster cluster de ned where cluster linear square clustering feature essen tially summary statistics cluster zeroth second momen cluster statistical view\nStatistical approac Conceptual clusterin clustering learning unlab jects duces classi cation jects\nUnlik tional clustering primarily ti es groups jects conceptual clustering further nding haracteristic descriptions group where group represen concept class\nmetho conceptual clustering adopt statistical approac probabilit measuremen determining concepts clusters\nCOBWEB opular simple metho incremen conceptual clustering input jects describ categorical attributev pairs\nImpro inference through conceptual clustering\nLearning observ ation Conceptual clustering\n\n++++\n\nWe provide some speciﬁc examples organized by whether the purpose of the clustering is understanding or utility\nThis idea is made a highlevel overview of clustering including a discussion of the various ap proaches to dividing objects into sets of clusters and the diﬀerent types of clusters\nFirst we further deﬁne cluster analysis illustrating why it is diﬃcult and explaining its relationship to other techniques that group data\nThen we explore two important topics ( ) diﬀerent ways to group a set of objects into a set of clusters and ( ) types of clusters\nThe greater the similarity ( or homogeneity ) within a group and the greater the diﬀerence between groups the better or more distinct the clustering\nNonetheless some work in graph partitioning and in image and market segmentation is related to cluster analysis\nDiﬀerent Types of Clusters Clustering aims to ﬁnd useful groups of objects ( clusters ) where usefulness is deﬁned by the goals of the data analysis\n<eos>"
    },
    {
        "key": "merged_topic_11_0",
        "label": "distance, level, measures, minimal, geometric /  concepts, algorithms, basic, analysis, wellseparated",
        "input": "<task:merge> <sos>\ndi ers tional clustering measures similarit based geometric distance\nAlgorithms based distance measures spherical clusters similar densit cluster could ortan algorithms detect clusters arbitrary Minimal requiremen domain wledge determine input parameters clustering algorithms require users input certain parameters cluster analysis desired clusters )\nwidelyused measures distance clusters follo where cluster jects distance jects Minim distance Maxim distance Root level st level CFtree structure\ndistance erage distance What diculties clustering\njects distributed cluster whose exemplar similar based distance measure\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms ( a ) Wellseparated clusters\nWe illustrate this with the set of two Cluster Analysis Basic Concepts and Algorithms ( a ) Optimal clustering\n( b ) Suboptimal clustering\nIn this way we obtain a set of initial Cluster Analysis Basic Concepts and Algorithms ( a ) Initial points\nCluster Analysis Basic Concepts and Algorithms ( a ) Iteration\nCluster Analysis Basic Concepts and Algorithms ( a ) Unequal sizes\nCluster Analysis Basic Concepts and Algorithms ( a ) Single link clustering\nCluster Analysis Basic Concepts and Algorithms ( a ) Original points\n<eos>"
    },
    {
        "key": "merged_topic_12_0",
        "label": "densit, yreac, ybased, hable, arbitraryshap /  circles, kmeans, clusters, adjacent, intersection",
        "input": "<task:merge> <sos>\nDensit habilit densit connectivit densit ybased clustering\nossible densit functions Examples terde ned clusters arbitraryshap clusters\nmetho sphericalshap clusters encoun dicult disco ering clusters arbitrary Other clustering metho based notion density\nTheir general cluster densit jects neigh exceeds threshold within cluster neigh radius least minim metho noise ( outliers ) disco clusters arbitrary DBSCAN ypical densit ybased metho clusters according densit threshold\nExample there jects cated space depicted rectangle cluster jects three clusters\ndistribution forms silhouettes encircled dotted grouping cluster ters\nMoreo clusters spherical erform ecause notion radius diameter oundary cluster\ndense region neigh de ned narro sparse region de ned widely tends result natural clusters comparison densit ybased metho DBSCAN ( describ Section instead global neigh Moreo densit region recorded edges\nDensit ybased metho disco clusters arbitrary densit ybased clustering metho These ypically regard clusters dense regions jects space separated regions densit ( represen noise )\nde nes cluster maxim densityc basic ideas densit ybased clustering de nitions\ndensit yreac jects there jects directly densit yreac hable densit yconne jects there densit yreac hable Densit habilit transitiv closure direct densit habilit relationship asymmetric\nDensit connectivit symmetric relation\nExample Consider represen radius circles Based de nitions jects since neigh taining least three directly densit yreac hable directly densit yreac hable Densit habilit densit connectivit densit ybased clustering\nBased previous observ ation densit yreac hable densit hable Similarly densit yreac hable densit yconnected\ndensit ybased cluster densit yconnected jects maxima densit yreac habilit tained cluster considered noise\ntains information equiv densit ybased clustering obtained range parameter settings\nDENCLUE Clustering based densit distribution functions DENCLUE ( DENsit ybased CLUstEring ) clustering metho based densit distribution functions\n\n++++\n\n( b ) Two clusters\n( c ) Four clusters\n( d ) Six clusters\nA triangular area ( cluster ) is adjacent to a rectangular one and there are two intertwined circles ( clusters )\n( Points in the intersection of the circles belong to both )\nAdd these two clusters to the list of clusters\n<eos>"
    },
    {
        "key": "merged_topic_12_1",
        "label": "densit, yreac, ybased, hable, arbitraryshap /  circles, kmeans, clusters, adjacent, intersection",
        "input": "<task:merge> <sos>\nmathematical function called in uenc function describ impact within neigh erall densit space deled analytically in uence function clusters determined mathematicall tifying density actors where densit attractors maxima erall densit function\ndi eren tiable in uence function algorithm guided gradien determine densit attractor square Gaussian densit functions Based these notions enterde ne cluster arbitr aryshap cluster formally de ned\narbitraryshap cluster densit yextracted densit function threshold where Examples terde ned clusters arbitraryshap clusters ottom there exists region another densit function along Examples terde ned arbitraryshap clusters What major advantages DENCLUE arison other clustering algorithms gener There eral solid mathematical foundation generalizes other clustering metho including partitionbased hierarc hical calit ybased metho clustering erties large amoun noise compact mathematical description arbitrarily clusters highdimensional sets cells information cells actually manages these cells treebased access structure signi can faster in uen algorithms DBSCAN factor metho requires careful selection densit parameter noise threshold selection parameters signi can in uence qualit clustering results\nerimen studies eCluster found erform CLARANS DBSCAN terms eciency clustering qualit eCluster applicable wdimensional data\ndensit ybased metho cluster jects based notion densit either clusters according densit neigh jects DBSCAN ) according densit function DENCLUE )\ndelbased metho othesizes clusters ypical delbased metho include statistical approac AutoClass COBWEB CLASSIT neural approac ersons noise could another ersons signal\ndiagram illustrate constan MinPts alue densityb clusters higher densit neigh radius ) completely tained densit yconnected obtained densit Human e ectiv judging qualit clustering metho odimensional data\n\n++++\n\nKmeans and Diﬀerent Types of Clusters Kmeans and its variations have a number of limitations with respect to ﬁnding diﬀerent types of clusters\n( b ) Three Kmeans clusters\n( b ) Two Kmeans clusters\n( b ) Nested cluster diagram\n( b ) Three clusters found by DBSCAN\n( c ) Three clusters found by Kmeans\n( d ) Three clusters found by complete link\n<eos>"
    },
    {
        "key": "merged_topic_13_0",
        "label": "structures, structure, criterion, erformed, tries /  analysis, groups, concepts, divides, exploratory",
        "input": "<task:merge> <sos>\nWhat typic applic ations clustering\nclustered tains jects represen ersons houses cumen tries memorybased clustering algorithms ypically erate either follo structures\nclustering erations erformed structure tized space )\nfollo sections examine clustering metho detail\nypically squarederror criterion used de ned where squareerror jects database space represen ject cluster ultidim ensional criterion tries resulting clusters compact separate ossible\nstructure clustering summarize jects discarded compressed\nerformance CLARANS further impro exploring spatial structures Rtrees cusing hniques\ne ectiv incremen dynamic clustering incoming jects\nLets closer emen tioned structures\nconstruct di eren clusterings ultaneously jects should cessed eci c order\nexample habilit simple dimensional presen general erview structured clustered\nularit cessing increase substan tially ottom structure coarse reduce qualit cluster analysis\neCluster Clustering using transformation eCluster ultiresolution clustering algorithm summarizes osing ultidim sional structure space\nFirst vides unsup ervised clustering\nHence conceptual clustering ostep cess clustering erformed follo haracterization\nurthermore classi cation heigh balanced input data cause space complexit degrade dramatically CLASSIT extension COBWEB incremen clustering alued ) data\nclustering constrained factors olving cation bridges a ect accessibilit Additional constrain limitati district forming region\n\n++++\n\nCluster Analysis Basic Concepts and Algorithms Cluster analysis divides data into groups ( clusters ) that are meaningful useful or both\nThe following are some examples Cluster Analysis Basic Concepts and Algorithms Biology\nWhat Is Cluster Analysis\nCluster analysis groups data objects based only on information found in the data that describes the objects and their relationships\nCluster analysis is related to other techniques that are used to divide data objects into groups\nRoad Map to introduce many of the concepts involved in cluster analysis\nMany times cluster analysis is conducted as a part of an exploratory data analysis\n<eos>"
    },
    {
        "key": "merged_topic_14_0",
        "label": "densit, yreac, ybased, hable, arbitraryshap /  dendrogram, height, shows, nested, joined",
        "input": "<task:merge> <sos>\nDensit habilit densit connectivit densit ybased clustering\nossible densit functions Examples terde ned clusters arbitraryshap clusters\nmetho sphericalshap clusters encoun dicult disco ering clusters arbitrary Other clustering metho based notion density\nTheir general cluster densit jects neigh exceeds threshold within cluster neigh radius least minim metho noise ( outliers ) disco clusters arbitrary DBSCAN ypical densit ybased metho clusters according densit threshold\nExample there jects cated space depicted rectangle cluster jects three clusters\ndistribution forms silhouettes encircled dotted grouping cluster ters\nMoreo clusters spherical erform ecause notion radius diameter oundary cluster\ndense region neigh de ned narro sparse region de ned widely tends result natural clusters comparison densit ybased metho DBSCAN ( describ Section instead global neigh Moreo densit region recorded edges\nDensit ybased metho disco clusters arbitrary densit ybased clustering metho These ypically regard clusters dense regions jects space separated regions densit ( represen noise )\nde nes cluster maxim densityc basic ideas densit ybased clustering de nitions\ndensit yreac jects there jects directly densit yreac hable densit yconne jects there densit yreac hable Densit habilit transitiv closure direct densit habilit relationship asymmetric\nDensit connectivit symmetric relation\nExample Consider represen radius circles Based de nitions jects since neigh taining least three directly densit yreac hable directly densit yreac hable Densit habilit densit connectivit densit ybased clustering\nBased previous observ ation densit yreac hable densit hable Similarly densit yreac hable densit yconnected\ndensit ybased cluster densit yconnected jects maxima densit yreac habilit tained cluster considered noise\ntains information equiv densit ybased clustering obtained range parameter settings\nDENCLUE Clustering based densit distribution functions DENCLUE ( DENsit ybased CLUstEring ) clustering metho based densit distribution functions\nmathematical function called in uenc function describ impact within neigh erall densit space deled analytically in uence function clusters determined mathematicall tifying density actors where densit attractors maxima erall densit function\n\n++++\n\nThe shapes of the markers indicate cluster membership\nshows the nested clusters as a sequence of nested ellipses where the numbers shows the same information but as a dendrogram\n<eos>"
    },
    {
        "key": "merged_topic_14_1",
        "label": "densit, yreac, ybased, hable, arbitraryshap /  dendrogram, height, shows, nested, joined",
        "input": "<task:merge> <sos>\ndi eren tiable in uence function algorithm guided gradien determine densit attractor square Gaussian densit functions Based these notions enterde ne cluster arbitr aryshap cluster formally de ned\narbitraryshap cluster densit yextracted densit function threshold where Examples terde ned clusters arbitraryshap clusters ottom there exists region another densit function along Examples terde ned arbitraryshap clusters What major advantages DENCLUE arison other clustering algorithms gener There eral solid mathematical foundation generalizes other clustering metho including partitionbased hierarc hical calit ybased metho clustering erties large amoun noise compact mathematical description arbitrarily clusters highdimensional sets cells information cells actually manages these cells treebased access structure signi can faster in uen algorithms DBSCAN factor metho requires careful selection densit parameter noise threshold selection parameters signi can in uence qualit clustering results\nerimen studies eCluster found erform CLARANS DBSCAN terms eciency clustering qualit eCluster applicable wdimensional data\ndensit ybased metho cluster jects based notion densit either clusters according densit neigh jects DBSCAN ) according densit function DENCLUE )\ndelbased metho othesizes clusters ypical delbased metho include statistical approac AutoClass COBWEB CLASSIT neural approac ersons noise could another ersons signal\ndiagram illustrate constan MinPts alue densityb clusters higher densit neigh radius ) completely tained densit yconnected obtained densit Human e ectiv judging qualit clustering metho odimensional data\n\n++++\n\nThe height at which two clusters are merged in the dendrogram reﬂects the distance of the two clusters\n( b ) Single link dendrogram\nis and that is the height at which they are joined into one cluster in the dendrogram\n( b ) Complete link dendrogram\n( b ) Group average dendrogram\n( b ) Wards dendrogram\n<eos>"
    },
    {
        "key": "merged_topic_15_0",
        "label": "users, groups, customer, attributes, dicult /  greater, types, group, aims, highlevel",
        "input": "<task:merge> <sos>\nbusiness clustering eters disco distinct groups their customer bases haracterize customer groups based hasing patterns\nAbilit di eren attributes algorithms designed cluster albased umerical ) data\nburdens users qualit clustering dicult trol\nhallenging groups clustering satisfy eci ed constrain terpretabi usabilit Users clustering results terpretable comprehensible usable\napplications categorical attributes necessit users Clustering jects based means metho cluster ecify clusters disadv tage\nattributes assigned cluster predicted attributes clusters exemplar\nconsiders exceptions hidden aggregated groupb detection exceptions dicult since searc space ypically large particularly there dimensions olving concept hierarc Summary cluster collection jects similar another within cluster dissimilar jects other clusters\n\n++++\n\nWe provide some speciﬁc examples organized by whether the purpose of the clustering is understanding or utility\nThis idea is made a highlevel overview of clustering including a discussion of the various ap proaches to dividing objects into sets of clusters and the diﬀerent types of clusters\nFirst we further deﬁne cluster analysis illustrating why it is diﬃcult and explaining its relationship to other techniques that group data\nThen we explore two important topics ( ) diﬀerent ways to group a set of objects into a set of clusters and ( ) types of clusters\nThe greater the similarity ( or homogeneity ) within a group and the greater the diﬀerence between groups the better or more distinct the clustering\nNonetheless some work in graph partitioning and in image and market segmentation is related to cluster analysis\nDiﬀerent Types of Clusters Clustering aims to ﬁnd useful groups of objects ( clusters ) where usefulness is deﬁned by the goals of the data analysis\n<eos>"
    },
    {
        "key": "file1_topic_16_0",
        "label": "metho, delbased, clustering, study, including",
        "input": "<task:clean> <sos>\nclustering analysis alscaled ariables Binary ariables Nominal ordinal ratioscaled ariables ariables mixed categorization clustering metho artitioning metho Classical partitioning metho means medoids artitioning metho large databases medoids CLARANS Hierarc hical metho Agglomerativ divisiv hierarc hical clustering Balanced Iterativ Reducing Clustering using Hierarc CURE Clustering Using REpresen tativ CHAMELEON hierarc hical clustering algorithm using dynamic deling Densit ybased metho DBSCAN densit ybased clustering metho based connected regions sucien densit OPTICS Ordering Clustering Structure DENCLUE Clustering based densit distribution functions Gridbased metho STING atistical INformation approac eCluster Clustering using transformation CLIQUE Clustering highdimensional space delbased clustering metho Statistical approac Neural approac Outlier analysis Statisticalbased outlier detection Distancebased outlier detection Deviationbased outlier detection Summary means algorithm\nClustering areas including mining statistics biology learning\nstudy compute dissimilarities jects represen arious attribute ariable study clustering hniques organized follo categories artitioning metho metho densityb metho gridb metho delb metho Clustering outlier ction forms topic hapter\nCluster analysis widely umerous applications including pattern recognition analysis image cess researc clustering sparse regions therefore disco erall distribution patterns teresting correlations among attributes\nclassify cumen information disco mining function cluster analysis standalone insigh distribution data observ haracteristics cluster particular clusters further analysis\napplications require clustering other data binary categorical ( nominal ) ordinal data mixtures these Disco clusters arbitrary clustering algorithms determine clusters based Euclidean Manhattan distance measures\nhallenging cluster jects highdimensional space ecially considering highdimensional space sparse highly Constrain tbased clustering Realw applications erform clustering under arious kinds constrain cations automatic stations decide this cluster households while considering constrain orks customer requiremen region\nortan study application in uence selection clustering metho these requiremen mind study cluster analysis ceeds follo First study di er in uence clustering metho Second presen general categorization clustering metho study clustering metho detail including partitioning metho hierarc hical metho densit ybased metho gridbased metho delbased metho examine clustering highdimensional space outlier analysis\ndissimilarit jects computed ariables describing jects di eren categorization clustering metho There exist large clustering algorithms literature\nhoice clustering algorithm ailable particular application\n<eos>"
    },
    {
        "key": "file1_topic_16_1",
        "label": "metho, delbased, clustering, study, including",
        "input": "<task:clean> <sos>\nscalable clustering algorithms CURE based tegrated approac Hierar hical clustering metho studied Section Densit ybased metho partitioning metho cluster jects based distance jects\nGridbased clustering metho studied Section delbased metho delbased metho othesize clusters delbased algorithm clusters constructing densit function re ects spatial distribution leads automatically determining clusters based standard statistics taking noise outliers accoun yielding robust clustering metho delbased clustering metho studied Section clustering algorithms tegrate ideas clustering metho sometimes dicult classify algorithm uniquely elonging clustering metho category urthermore applications clustering criteria require tegration clustering hniques\nalgo rithms tegrate ideas clustering metho Outlier analysis ypically clustering describ Section artitioning metho database jects clusters form partitioning algorithm organizes jects partitions where partition represen cluster\nscalabilit iterativ clustering algorithm includes clustering features compressible jects jects retained memory thereb turning secondarymemory based algorithm memorybased algorithm\nlimited amoun memory ortan consideration minimi required applies ultiphase clustering hnique single yields basic clustering additional scans further impro qualit computation complexit algorithm where jects clustered\naccuracy clustering result degraded simplicit metho delbased clustering metho delbased clustering metho attempt optimize mathematical metho often based assumption generated mixture underlying probabilit distributions\ngrouping ysical abstract jects classes similar jects called clustering Cluster analysis applications including customer segmen tation pattern recognition logical studies spatial analysis cumen classi cation others\nqualit clustering assessed based measure dissimil jects computed arious data including intervalsc ariables binary ariables nominal dinal atiosc ariables binations these ariable Clustering dynamic researc mining large clustering algorithms These algorithms categorized artitioning metho metho densityb metho gridb metho delb metho partitioni metho creates initial partition where partitions construct ative ation chnique attempts impro partitioning jects group another\nBased learned clustering metho design clustering metho clusters large e ectiv ecien Automatic eller hines region satisfy constrain Households places clustered ypically assigned cluster\n<eos>"
    },
    {
        "key": "file1_topic_16_2",
        "label": "metho, delbased, clustering, study, including",
        "input": "<task:clean> <sos>\nseminal papaers delbased clustering Dietteric Conceptual clustering duced halski Stepp Other examples statistical clustering approac include COBWEB Fisher CLASSIT Gennari Langley Fisher AutoClass Cheeseman Stutz Studies neural approac include etitiv learning Rumelhart Zipser ( selforganizing feature maps ) Kohonen Outlier detection analysis categorized three approac statistical approac distance based approac deviationbased approac statistical approac discordancy tests describ Barnett Lewis Distancebased outlier detection describ Knorr sequen problem approac deviationbased outlier detection duced Arning Ragha Megiddo duced disco erydriv metho tifying exceptions large ultidim ensional using Dense units found dimensions Salary ation tersected order candidate searc space dense units higher dimensionalit animal P ( C ) P ( scalesC ) P ( scalesC ) P ( C ) mammalbird P ( C ) P ( hairC ) mammal P ( C ) P ( hairC ) P ( C ) P ( feathersC ) amphibian P ( C ) P ( moistC ) classi cation tree\n<eos>"
    },
    {
        "key": "file1_topic_17_0",
        "label": "hierarc, hical, step, agglomerativ, split",
        "input": "<task:clean> <sos>\nAgglomerativ divisiv hierarc hical clustering\nCluster analysis ortan activit Early hildho learns distinguish dogs animals uously impro conscious classi cation hemes\niterativ cation hnique attempts impro partitioning jects group another\nhierarc hical metho classi ed either agglomer ative divisive based hierarc hical decomp osition formed\nagglomer ative called ottomup approac starts forming separate group\nsuccessiv merges jects groups close another groups merged topmost hierarc termination condition holds\nHierarc hical metho su er ( merge split ) done undone\ntageous iterativ cation hierarc hical agglomeration using hierar hical agglomerativ algorithm re ning result using iterativ cation\nteresting strategy often yields results apply hierarc hical agglomeration algorithm determine clusters initial classi cation iterativ cation impro classi cation\nHierarc hical metho hierarc hical clustering metho grouping jects clusters\nHierarc hical clustering metho further classi ed agglomer ative divisive hierarc hical clustering ending whether hierarc hical decomp osition formed ottomup topdo fashion\nqualit hierarc hical clustering step step step step step c d e a b c d e step step step step step agglomerative divisive Agglomerativ divisiv hierarc hical clustering jects metho su ers inabilit erform adjustmen merge split decision executed\nRecen studies emphasized tegration hierarc hical agglomeration iterativ cation metho Agglomerativ divisiv hierarc hical clustering general there hierarc hical clustering metho Agglomerativ hierarc hical clustering ottomup strategy starts placing cluster merges these atomic clusters larger larger clusters jects single cluster certain termination conditions satis ed\nhierarc hical clustering metho elong category di er their de nition tercluster similarit Divisiv hierarc clusterin topdo strategy agglomerativ hierarc hical clustering starting jects cluster\nExample application Gglomerativ NESting ) agglomerativ hierarc hical clustering metho DIANA ( DIvisia ANAlysis ) divisiv hierarc hical clustering metho jects Initially places cluster clusters merged stepb ystep according criterion\ncluster merging jects tually merged cluster\neither agglomerativ divisiv hierarc hical clustering ecify desired clusters termination condition\nhierarc hical clustering metho though simple often encoun diculties regarding selection merge split decision critical ecause group jects merged split erate newly generated clusters\nmerge split decisions hosen step qualit clusters\nMoreo metho scale since decision merge split needs examine aluate jects clusters\n<eos>"
    },
    {
        "key": "file1_topic_17_1",
        "label": "hierarc, hical, step, agglomerativ, split",
        "input": "<task:clean> <sos>\nBalanced Iterativ Reducing Clustering using Hierarc ( Balanced Iterativ Reducing Clustering using Hierarc hies ) tegrated hierarc hical clustering metho duces concepts clusterin feature clustering feature tree ) summarize cluster represen tations\nheigh tbalanced stores clustering features hierarc hical clustering\nexample de nition nonleaf descenden hildren\nnonleaf store their hildren summarize clustering information their hildren\nemplo hierarc hical clustering algorithm adopts middle ground agglomerativ ottomup ) divisiv ( topdo approac Instead using single troid represen cluster represen tativ space hosen\nconstructs sparse graph similarit matrix using similarit threshold concept shared neigh erforms hierarc hical clustering algorithm sparse graph\nagglomerativ hierarc hical clustering algorithm clusters eatedly bining these clusters\nThese merging splittin incorp orated hosts considered merging single class\nThese decisions based category utilit merging splitting erators COBWEB erform bidirectional searc merge previous split\nypical partition metho include means medoids CLARANS their impro hierarc metho creates hierarc hical decomp osition jects\nmetho classi ed either agglomer ative ottomup ) divisive ( topdown ) based hierarc hical decomp osition formed\nensate rigidit merge split hierarc hical agglomeration often grates other clustering hniques iterativ cation\nOptimization simpli cation hierarc hical clusterings\n<eos>"
    },
    {
        "key": "file1_topic_18",
        "label": "chameleon, deling, dynamic, ignore, hemes",
        "input": "<task:clean> <sos>\nCHAMELEON Hierarc hical clustering based knearest neigh dynamic deling\nCHAMELEON hierarc hical clustering algorithm using dynamic deling CHAMELEON clustering algorithm explores dynamic deling hierarc hical clustering\nCHAMELEON deriv based observ ation eakness hierarc hical clustering algorithms related hemes ignore information aggregate terconnectivit jects di eren clusters whereas related hemes ignore information closeness clusters while emphasizing their terconnectivit CHAMELEON work\nCHAMELEON hierarc hical clustering algorithm using dynamic deling\n<eos>"
    },
    {
        "key": "file1_topic_19",
        "label": "optics, ordering, augmen, teractiv, computes",
        "input": "<task:clean> <sos>\nOPTICS terminology Cluster ordering OPTICS\nOPTICS densit ybased metho computes augmen clustering ordering automatic teractiv cluster analysis\nOPTICS Ordering Clustering Structure Although DBSCAN densit ybased clustering algorithm describ Section cluster jects input parameters still onsibilit selecting parameter alues disco acceptable clusters\nercome dicult cluster ordering metho called OPTICS ( Ordering Clustering Structure ) osed\nRather clustering explicitly OPTICS computes augmen cluster dering automatic teractiv cluster analysis\nthese values OPTICS algorithm creates ordering jects database additionally storing coredistance suitable habilit ydistance ject\nmetho built follo ideas in uence formally deled using Cluster ordering OPTICS\nOptics Ordering clustering structure\n<eos>"
    },
    {
        "key": "file1_topic_20",
        "label": "sting, hical, hierarc, structure, atistical",
        "input": "<task:clean> <sos>\nhierarc hical structure STING clustering\nartitioningbased clustering metho studied depth Section Hierarc hical metho hierarc hical metho creates hierarc hical decomp osition jects\npromising direction impro clustering qualit hierarc hical metho tegrate hierarc hical clustering other clustering hniques ultiple phase clustering\nSTING atistical INformation approac STING atistical INformation Grid ) gridbased ultiresolution clustering hnique spatial ( i ) st layer ith layer hierarc hical structure STING clustering\nThese statistical parameters useful query cessing describ hierarc hical structure STING clustering\nAfter generating hierarc hical structure query cessing where total cells usually smaller Since STING ultiresolution approac erform cluster analysis qualit STING clustering ularit structure\n<eos>"
    },
    {
        "key": "file1_topic_21",
        "label": "binary, ariables, ariable, states, nominal",
        "input": "<task:clean> <sos>\nables tingency table binary ariables\nbinary ariable states where means ariable absen means presen ariable smoker describing patien instance indicates patien while indicates patien reating binary ariables alscaled misleading clustering results\nbinary ariables though tingency table where ariables equal jects ariables equal ariables equal equal ariables equal jects total ariables where tingency table binary ariables\nbinary ariable symmetric states equally aluable carry there preference outcome should example could attribute gender states female\nasymmetric binary ariables agreemen ositiv considered signi can zeros negativ Therefore binary ariables often considered monary state )\nsymmetric asymmetric binary ariables mixed ariables approac describ Section applied\nExample Dissimil binary ariables\nNominal ariables nominal ariable generalization binary ariable states\nexample nominal ariable states pink blue\nstates nominal ariable states denoted letters tegers Notice tegers handling represen eci c ordering\nNominal ariables asymmetric binary ariables creating binary ariable nominal states\nstate alue binary ariable represen state while remaining binary ariables example nominal ariable olor binary ariable created colors listed color ariable while remaining ariables dissimilarit ecien calculated using metho discussed Section Ordinal ariables discrete ordinal ariable resem nominal ariable except states ordinal ordered meaningful sequence\n<eos>"
    },
    {
        "key": "file1_topic_22",
        "label": "test, trait, relational, table, gender",
        "input": "<task:clean> <sos>\nrelational table taining mostly binary attributes\nrelational table taining binary attributes enpal service\npatien record table tains attributes name gender fever ough test test test test where jectid gender symmetric attribute remaining attributes asymmetric binary gender cough test test test test relational table taining mostly binary attributes\ngender trait trait trait trait relational table taining binary attributes enpal service\n<eos>"
    },
    {
        "key": "file1_topic_23",
        "label": "distance, function, manhattan, euclidean, uence",
        "input": "<task:clean> <sos>\nOften distance measures used\nThese measures include Euclide Manhattan Minkowski distanc What intervalsc variables\nopular distance measure Euclidean distance de ned where pdimensional jects\nAnother ellkno metric Manhattan distance de ned Euclidean distance Manhattan distance satisfy follo mathematic requiremen distance function states distance nonnegativ states distance itself states distance symmetric function triangular quality states going directly space making detour other distance generalization Euclidean distance Manhattan distance\nrepresen Manhattan distance Euclidean distance ariable assigned according erceiv ortance weighte Euclidean distance computed follo applied Manhattan distances\nin uence function function de ned terms basic in uence function principle in uence function arbitrary function determine distance jects neigh should re exiv symmetric Euclidean distance function ( Section squar in uenc function other Gaussian in uenc function Gauss d ( xy densit function de ned in uence functions jects densit function de ned example densit function results Gaussian in uence function Gaussian d ( xx densit function de ne adient function density actor maxima erall densit function\nCompute Manhattan distanc jects\ndistance function Euclidean distance\n<eos>"
    },
    {
        "key": "file1_topic_24",
        "label": "databases, large, scalabilit, database, random",
        "input": "<task:clean> <sos>\nhapter learn requiremen clustering metho erating large amoun data\nScalabilit clustering algorithms small taining jects large database milli jects\nHighly scalable clustering algorithms needed\nThese heuristic clustering metho nding sphericalshap clusters small medium sized databases\nartitioning metho large databases medoids CLARANS ecient doids algorithm sets\nThese structures clustering metho scalabilit large databases\nscales large databases without sacri cing clustering qualit handle large databases emplo bination random sampling partitioning random sample partitioned partition partially clustered\neletbased clustering fast computational complexit where jects database\nuseful clustering dimensional large databases\nsu ers similar problems COBWEB suitable clustering large database data\nCure ecien clustering algorithm large databases\necien approac clustering large ultim databases noise\n<eos>"
    },
    {
        "key": "file1_topic_25",
        "label": "closeness, ertices, ternal, ords, edges",
        "input": "<task:clean> <sos>\ncluster collection jects similar another within cluster dissimilar jects other clusters\nclusters compact clouds rather separated another\nother ords there strict oundaries clusters\ncorresp onding cluster Lets example orks\nneigh other ords cluster similarit based di eren clusters neigh common\nclus tering cess clusters merged terconnectivit closeness ximit clusters highly related ternal terconnectivit closeness jects within clusters\nrelativ closeness clusters absolute closeness normalized ternal closeness clusters de ned where erage edges connect ertices ertices erage edges elong mincut bisector cluster CHAMELEON disco ering arbitrarilyshap clusters qualit DBSCAN\nneigh tains cluster created\nexemplar protot cluster necessarily corresp particular example ject\n<eos>"
    },
    {
        "key": "file1_topic_26",
        "label": "spatial, editors, mining, ester, august",
        "input": "<task:clean> <sos>\nclustering under vigorous elopmen tributing areas researc include mining statistics learning spatial database hnology biology eting\nalgorithm regions sucien densit clusters disco clusters arbitrary spatial databases noise\nPiatetskyShapiro urusam editors dvanc overy Mining AAAIMIT Press Ester Kriegel Sander densit ybased algorithm disco ering clusters large spatial databases\novery Mining ortland Oregon August Ester Kriegel wledge disco large spatial databases cusing hniques ecien class ti cation\nhalski onell hell editors Machine arning arti cial intel ligenc Mateo Morgan Kaufmann Ecien e ectiv clustering metho spatial mining\nBases August STING statistical information approac spatial mining\n<eos>"
    },
    {
        "key": "file1_topic_27",
        "label": "rousseeu, kaufman, extended, hniques, metho",
        "input": "<task:clean> <sos>\nActiv themes researc alability clustering metho e ectiv eness metho clustering omplex data highdimensional clustering hniques metho clustering numeric goric large databases\nclustering algorithms handling wdimensional data olving three dimensions\nHuman judging qualit clustering three dimensions\nnding clusters complex clustering large sets partitioningbased metho extended\nMetho viewing clustering structures high dimensional data\ndesign visualization metho umans visualize clusters judge clustering qualit threedimensional data\nBibliographic notes Clustering metho discussed textb Kaufman Rousseeu Metho bining ariables di eren single dissimilarit matrix later extended Kaufman Rousseeu partitioning metho means algorithm duced MacQueen medoids algo rithms CLARA Kaufman Rousseeu algorithm Huang while ectation Maximization ) algorithm duced Lauritzen CLARANS algorithm Ester Kriegel hniques further impro erformance CLARANS using ecien spatial access metho Rtree cusing hniques\nBases thens Greece Zhang Ramakrishnan ecien clustering metho large databases\n<eos>"
    },
    {
        "key": "file1_topic_28",
        "label": "application, mining, researc, requiremen, preparation",
        "input": "<task:clean> <sos>\nClustering hallenging researc where applications their ecial requiremen follo ypical requiremen clustering mining\nAdditional researc needed application conceptual clustering metho mining\napplication example follo cases application clustering mining function\napplication clustering prepro cessing preparation other mining tasks\n<eos>"
    },
    {
        "key": "file1_topic_29",
        "label": "sensitiv, algorithms, eren, qualit, input",
        "input": "<task:clean> <sos>\nclustering results often quite sensitiv input parameters\nclustering algorithms sensitiv clusters qualit Insensitivi order input records clustering algorithms sensitiv order input data data presen di eren orderings algorithm generate dramatically di eren clusters\njects complexit sensitive usersp ameters sample size desir clusters shrinking action sensitivit analysis although parameters aried without impacting qualit clustering parameter setting general signi can in uence results\nActually problem ciated other clustering algorithms\nalgorithms sensitiv parameter alues sligh di eren settings di eren clusterings data\nHere clustering qualit solely function individual jects\nclustering clustering general ) cluster though feature detects regularit jects\nconstrain clustering algorithms di ed onstr aintb clustering\n<eos>"
    },
    {
        "key": "file1_topic_30",
        "label": "ecially, highdimensional, alues, empirically, arameters",
        "input": "<task:clean> <sos>\narameters often determine ecially taining highdimensional jects\ncessing highdimensional require jects case\nparameter settings usually empirically dicult determine ecially realw orld highdimensional sets\necially attributes large alues since their space complexities attributes alues attribute\nultidim ensional analyzed particular rather ombination dimension alues extreme\n<eos>"
    },
    {
        "key": "file1_topic_31",
        "label": "lbyc, noisy, trivial, outliers, jectb",
        "input": "<task:clean> <sos>\nAbilit noisy data realw databases outliers missing unkno erroneous data\nproblem de ning outliers trivial\nWhat using visualization metho outlier ction\nalgorithm outliers lbyc rather jectb basis\nKnorr uni ed notion outliers erties computation\n<eos>"
    },
    {
        "key": "file1_topic_32",
        "label": "order, insensitiv, input, sensitiv, sensitivity",
        "input": "<task:clean> <sos>\nortan algorithms insensitiv order input\nmight algorithm diminish sensitivity\nneigh randomly tried restricted usersp eci ed parameter\nalgorithm sensitiv userde ned parameters\ninsensitiv order input tuples presume canonical distribution\nerators tioned highly sensitiv input order ject\nalleviate ossible in uence input order results eated times di eren random ordering subsets\n<eos>"
    },
    {
        "key": "file1_topic_33",
        "label": "matrix, dissimilarit, efore, onemo, columns",
        "input": "<task:clean> <sos>\nSince matrix Measures dissimilarit discussed throughout section\nmatrix often called matrix whereas dissimilarit matrix called onemo matrix since columns former represen di eren tities while those latter represen clustering algorithms erate dissimilarit matrix\npresen matrix transformed dissimilarit matrix efore applying clustering algorithms\npartitioning metho still erformed based principle minim dissimilarities corresp onding reference forms basis medoids metho basic strategy medoids clustering algorithms clusters jects arbitrarily nding represen tativ medoid ) cluster\n<eos>"
    },
    {
        "key": "file1_topic_34",
        "label": "ariables, dissimilarit, ariable, compute, ranking",
        "input": "<task:clean> <sos>\nsection discuss dissimilarit computed jects describ intervalsc ariables binary ariables nominal dinal atiosc ariables binations these ariable dissimilarit later compute clusters jects\ndescrib distance measures commonly computing dissimilarit jects describ ariables\nsimilarit based ariables called nonin arian similari nonin arian similariti ellkno ecien Jaccard ecien de ned where negativ considered unimp ortan ignored computation\ndissimilarit jects computed using simple approac where matches ariables state ) total ariables\nOrdinal ariables useful registering jectiv assessmen qualities can not measured jectiv example professional ranks often umerated sequen order assistan ciate full\nexample relativ ranking particular often essen actual alues particular measure\nThese ordered states de ne ranking dinal variables treatmen ordinal ariables quite similar alscaled ariables computing dissimilarit jects\nordered states represen ranking Replace corresp onding rank Since ordinal ariable di eren states often necessary range ariable ariable equal replacing ariable Dissimilarit computed using distance measures describ Section scaled ariables using represen ject\nThere three metho handle ratioscaled ariables computing dissimilarit jects\nhnique bines di eren ariables single dissimilarit matrix bringing meaningful ariables common scale tains ariables mixed dissimilarit jects de ned where indicator either missing there measuremen ariable ariable asymmetric binary otherwise tribution ariable dissimilarit computed enden binary nominal otherwise albased where nonmissing jects ariable ordinal ratioscaled compute ranks treat alscaled\nBrie y outline compute dissimilarit jects describ follo ariables asymmetric binary ariables nominal ariables ratioscaled ariables umerical alscaled ) ariables follo measuremen ariable Standardize ariable follo wing Compute absolute deviation Compute zscore measuremen jects represen follo tuples Compute Euclide distanc jects\n<eos>"
    },
    {
        "key": "file1_topic_35",
        "label": "ordinal, anks, discretization, treat, tities",
        "input": "<task:clean> <sos>\nassigned increase e ect assign greater ariables larger states\nOrdinal ariables obtained discretization alscaled tities splitting range classes\nalues ordinal ariable anks\nexample ordinal ariable states\nariable ordinal ariables describing jects\nordinal treat their ranks alued\n<eos>"
    },
    {
        "key": "file1_topic_36",
        "label": "complexit, index, computational, nestedlo, computation",
        "input": "<task:clean> <sos>\nrigidit useful leads smaller computation costs orrying binatorial di eren hoices\nmetho relativ scalable ecien cessing large ecause computational complexit algorithm where total jects clusters iterations\nlarge alues computation ecomes costly Which metho obust dians\ncomputational complexit CLARANS where jects\nalgorithm complexit where dimensionalit jects indexbased gorithm scales increases\ncomplexit aluation searc accoun though building index itself computationally tensiv Nestedlo algorithm nestedlo algorithm computational complexit index based algorithm index structure construction tries minimi IOs\nalgorithm linear complexit where jects input vided iterations complexit based notion computation dissimilarit function incremen dissimilarit subset sequence computed previous subset\n<eos>"
    },
    {
        "key": "file1_topic_37",
        "label": "sales, decem, exception, exceptions, increase",
        "input": "<task:clean> <sos>\nproblem hniques can not correct erroneous decisions\nsequen tially compares jects while second emplo approac Sequen exception hnique sequen exception hnique ulates umans distinguish usual jects among series osedlylik jects\nde ning function complicated nature exceptions ance\napproac overydriven explor ation where precomputed measures indicating exceptions guide analysis aggregation\nconsidered exception signi can di eren ected alue based statistical ected considered function aggregated computed using alue\nmeasure re ect exceptions ccurring detailed lower levels where these exceptions visible curren considers ariations patterns measure across dimensions elongs\nexample sales data viewing sales summarized visual cues notice increase sales Decem comparison other exception dimension\ndrillingdo Decem sales there similar increase sales other items during Decem Therefore increase total sales Decem exception dimension considered\n<eos>"
    },
    {
        "key": "file1_topic_38",
        "label": "gridbased, structure, tizes, metho, cells",
        "input": "<task:clean> <sos>\nDensit ybased clustering metho studied Section Gridbased metho Gridbased metho space cells structure\nGridbased metho gridbased clustering approac ultiresolution structure\ntizes space cells structure erations clustering erformed\ntopdo gridbased metho follo First within hierarc hical structure determined queryansw ering start\ngridbased metho tizes space cells structure erforms clustering structure\nSTING ypical example gridbased metho based statistical information stored cells\nBased Jaccard ecien compatible pair Brie y describ follo approac clustering metho partitioning metho hierarc hical metho densit ybased metho gridbased metho delbased metho examples case\ntree ecien robust access metho rectangles\n<eos>"
    },
    {
        "key": "file1_topic_39",
        "label": "clique, gridbased, ecluster, ybased, grid",
        "input": "<task:clean> <sos>\nSTING ypical example gridbased metho CLIQUE eCluster clustering algorithms gridbased densit ybased\nypical example gridbased approac include STING explores statistical information stored cells eCluster clusters jects using transform metho CLIQUE represen grid densit ybased approac clustering highdimensional space\neCluster gridbased densit ybased algorithm\nCLIQUE Clustering highdimensional space CLIQUE ( CLustering QUEst ) clustering algorithm tegrates densit ybased gridbased clustering\nCLIQUE eCluster clustering algorithms gridbased densit ybased\nCLIQUE Gehrk Gunopulos Ragha tegrated densit ybased grid based clustering metho clustering highdimensional data\n<eos>"
    },
    {
        "key": "file1_topic_40",
        "label": "redistribution, terminates, leading, ensiv, ossibly",
        "input": "<task:clean> <sos>\nNormally metho often terminates optim means metho applied cluster de ned\nredistribution forms silhouettes encircled dashed iterates leading tually redistribution jects cluster ccurs terminates\ncessing costly means metho metho require ecify clusters\nmetho incremen inserted closest cluster )\ndiameter cluster stored after insertion larger threshold alue ossibly other split\nMoreo probabilit distribution represen tation clusters quite ensiv store clusters\n<eos>"
    },
    {
        "key": "file1_topic_41",
        "label": "memory, regions, cellbased, compressible, discardable",
        "input": "<task:clean> <sos>\nrecen e ort scaling means algorithm based tifying three kinds regions data regions compressible regions tained memory regions discardable\nneither discardable compressible should etaine memory\nmemory needed storing larger memory smaller threshold eci ed rebuilt\ndivides memory bu er space logical carefully osing order loaded half eciency Cellbased algorithm computational complexit cellbased algorithm memoryresiden sets\n<eos>"
    },
    {
        "key": "file1_topic_42",
        "label": "medoids, medoid, clara, qualit, nonmedoids",
        "input": "<task:clean> <sos>\ncases function medoids clustering\nremaining clustered medoid similar\nstrategy iterativ replaces medoids nonmedoids qualit resulting clustering impro qualit estimated using function measures erage dissimilarit medoid cluster\nossible pairs jects analyzed where considered medoid other qualit resulting clustering calculated bination\nNotice searc medoids among whereas CLARA searc medoids among sample CLARA can not clustering sampled medoid among medoids\nexample medoids medoids selected during sampling CLARA clustering\nclustering obtained after replacing single medoid called neighb curren clustering\n<eos>"
    },
    {
        "key": "file1_topic_43",
        "label": "detection, distort, heuristics, jectbased, generalizes",
        "input": "<task:clean> <sos>\nRepresen tativ jectbased hnique medoids metho means algorithm sensitiv outliers since extremely large substan tially distort distribution data\nheuristics metho duced outliers impro qualit trees additional scans data\nsection instead examine computerbased metho outlier detection\ncomparison statisticalbased metho distancebased outlier detection generalizes uni es ideas ehind discordancy testing standard distributions\nsection study hniques deviationbased outlier detection\nArning Ragha linear metho deviation detection large databases\n<eos>"
    },
    {
        "key": "file1_topic_44",
        "label": "squareerror, replaced, calculates, causing, reassignmen",
        "input": "<task:clean> <sos>\nreassignmen ccurs di erence squareerror tributed function\nTherefore function calculates di er square error curren medoid replaced nonmedoid ject\ntotal negativ replaced andom since actual squareerror reduced\nject replaced causing greatest reduction squareerror\n<eos>"
    },
    {
        "key": "file1_topic_45",
        "label": "distancebased, detection, outlier, tests, indexbased",
        "input": "<task:clean> <sos>\nCLARANS enables detection outliers\nDistancebased outlier detection notion distancebased outliers duced limitatio statistical metho What distanc outlier\ndistancebased outlier parameters least fraction jects distance greater other ords rather relying statistical tests think distancebased outliers those jects enough neigh where neigh de ned based distance ject\nDistancebased outlier detection excessiv computation ciated tting observ distribution standard distribution selecting discordancy tests\necien algorithms mining distancebased outliers These outlined follo Indexbased algorithm indexbased algorithm ultidim ensional indexing struc tures Rtrees trees searc neigh within radius around ject\nDistancebased outlier detection requires parameters\nDeviationbased outlier detection Deviationbased outlier detection statistical tests distancebased measures exceptional jects\nComputerbased outlier analysis metho ypically follo either statistic distanc deviationb Exercises clustering\n<eos>"
    },
    {
        "key": "file1_topic_46",
        "label": "sizes, outliers, spherical, robust, geometry",
        "input": "<task:clean> <sos>\nercomes problem oring clusters spherical similar sizes robust outliers\nrepresen tativ cluster adjust geometry spherical shrinking condensing clusters helps e ects outliers\nTherefore robust outliers ti es clusters nonspherical ariance size\ninitial jects partitioned clusters outliers excluded duces qualit clusters existence outliers clusters complex di eren sizes\noutliers insensitiv order input require eci cation input parameters clusters neigh radius\n<eos>"
    },
    {
        "key": "file1_topic_47",
        "label": "categorical, direction, guha, rastogi, hical",
        "input": "<task:clean> <sos>\nalternativ agglomerativ hierarc hical clustering algorithm that unlik CURE suited clus tering categorical attributes\nAgglomerativ hierarc hical clustering GNES divisiv hierarc hical clustering DIANA duced Kaufman Rousseeu teresting direction impro clustering qualit hierarc hical clustering metho tegrate hierarc hical clustering distancebased clustering iterativ cation other nonhierarc hical clustering metho ypical studies direction include algorithm Zhang Ramakrishnan Guha Rastogi clustering categorical attributes ) Guha Rastogi CHAMELEON Karypis Kumar densit ybased clustering metho DBSCAN Ester Kriegel Sander while erst Breunig Kriegel cluster ordering metho Optics\nrobust clustering algorithm categorical attributes\nExtensions kmeans algorithm clustering large categorical alues\n<eos>"
    },
    {
        "key": "file1_topic_48",
        "label": "regions, query, dense, endent, nearb",
        "input": "<task:clean> <sos>\ndense region tends sparse region\nOtherwise relev cells retriev further cessed requiremen query What advantages STING other clustering metho There eral gridbased putation queryindep endent since statistical information stored represen summary information cell indep enden query structure facilitates parallel cessing incre dating metho eciency tage STING through database compute statistical parameters cells hence complexit generating clusters where total jects\nClusters ti ed searc dense regions domain\ndense regions original feature space attractors nearb inhibitors further means clusters automatically stand clear regions around them\n<eos>"
    },
    {
        "key": "file1_topic_49",
        "label": "indicated, winning, ordering, ecomes, active",
        "input": "<task:clean> <sos>\nordering represen densit ybased clustering structure data\nTherefore order ordering densit ybased clusters distance parameter alues\ninformation sucien extraction densit ybased clusterings distance smaller distance generating order\ncluster ordering represen graphically helps understanding\nwinning within cluster ecomes active ( indicated circle ) while others inactive ( indicated circles )\nwhose ector closest curren ecomes winning activ unit\n<eos>"
    },
    {
        "key": "file1_topic_50",
        "label": "coredistance, reachabilitydistance, unde, εmm, habilit",
        "input": "<task:clean> <sos>\nject coredistance unde ned\nhabilit another greater coredistance Euclidean distance ject habilit ydistance unde ned\nε mm εmm Core distance of p Reachabilitydistance ( p q ) εmm Reachabilitydistance ( p q ) d ( p q ) ε mm OPTICS terminology Example illustrates concepts coredistance habilit ydistance\ncoredistance distance fourth closest ject\n<eos>"
    },
    {
        "key": "file1_topic_51",
        "label": "resolution, transform, handles, ultiresolution, domain",
        "input": "<task:clean> <sos>\nsummary information ypically memory ultiresolution transform subsequen cluster analysis\napplying transform erted spatial domain ( where feature attribute space ) quency domain di eren scales resolutions ) natural clusters ecome distinguishable\nconforms requiremen clustering algorithm handles large ecien disco clusters arbitrary successfully handles Multiresolution feature space scale ( high resolution ) scale ( medium resolution ) scale resolution )\nresulting clusters mapping wlev features higherlev features\nSTING gridbased ultiresolution approac eCluster Sheikholeslami Chatterjee Zhang ultiresolution clustering approac transforms original feature space transform\n<eos>"
    },
    {
        "key": "file1_topic_52",
        "label": "dense, units, space, searc, candidate",
        "input": "<task:clean> <sos>\nsubspaces represen these dense units tersected andidate searc space dense units higher dimensionalit exist\nti cation actual dense units within candidate searc space based Apriori ciation mining general emplo prior wledge items searc space ortions space pruned\nTherefore generate candidate dense units dimensional space dense units found ) dimensional\ngeneral resulting space searc smaller original space\ndense units examined order determine clusters\n<eos>"
    },
    {
        "key": "file1_topic_53",
        "label": "utilit, category, partition, umerator, ected",
        "input": "<task:clean> <sos>\nwhere concepts categories forming partition tree\nother ords category utilit increase ected attribute alues correctly guessed partition ( where ected represen umerator Equation ected correct guesses wledge ( where second umerator )\ndecision based orarily placing computing category utilit resulting partition\nplacemen results highest category utilit should ject\ncompared computation based existing placed existing class class created based partition highest category utilit alue\n<eos>"
    },
    {
        "key": "file1_topic_54",
        "label": "ortland, fisher, august, overy, atial",
        "input": "<task:clean> <sos>\nMining overy ortland Oregon August Barnett Lewis\natial Datab ortland Maine August Fisher\novery Mining treal Canada Gennari Langley Fisher\nEngine ering Sydney Australia\n<eos>"
    },
    {
        "key": "file2_topic_55",
        "label": "stories, set, instead, data, generic",
        "input": "<task:clean> <sos>\nIn some cases however cluster analysis is only a useful starting point for other purposes such as data summarization\nHowever instead of applying the algorithm to the entire data set it can be applied to a reduced data set consisting only of cluster prototypes\nMany times objects in the data set may represent noise outliers or uninteresting background For example some newspaper stories may share a common theme such as global warming while other stories are more generic or oneofakind\nWe will later see an example in which this leads to a suboptimal clustering\nHowever these limitations can be overcome in some sense if the user is willing to accept a clustering that breaks the natural clusters into a data sets if we ﬁnd six clusters instead of two or three\nA key motivation is that almost every clustering algorithm will ﬁnd clusters in a data set even if that data set has no natural cluster structure\nThis information often can be used to improve the quality of a clustering\nAlso the data may consist of nested clusters\n<eos>"
    },
    {
        "key": "file2_topic_56",
        "label": "tree, nested, partitional, hierarchical, subclusters",
        "input": "<task:clean> <sos>\nEach category ( cluster ) can be broken into subcategories ( sub clusters ) producing a hierarchical structure that further assists a users exploration of the query results\nHierarchical versus Partitional The most commonly discussed distinc tion among diﬀerent types of clusterings is whether the set of clusters is nested Cluster Analysis Basic Concepts and Algorithms or unnested or in more traditional terminology hierarchical or partitional\nIf we permit clusters to have subclusters then we obtain a hierarchical clustering which is a set of nested clusters that are organized as a tree\nEach node ( cluster ) in the tree ( except for the leaf nodes ) is the union of its children ( subclusters ) and the root of the tree is the cluster containing all the objects\nIf we allow clusters to be nested then one interpretation of ( ad ) when taken in that order also form a hierarchical ( nested ) clustering with respectively and clusters on each level\nFinally note that a hierarchical clustering can be viewed as a sequence of partitional clusterings and a partitional clustering can be obtained by taking any member of that sequence ie by cutting the hierarchical tree at a particular level\n<eos>"
    },
    {
        "key": "file2_topic_57",
        "label": "prototype, graphbased, connected, prototypebased, contiguitybased",
        "input": "<task:clean> <sos>\nAd ditionally some clustering techniques characterize each cluster in terms of a cluster prototype ie a data object that is representative of the other ob jects in the cluster\nEach object is represented by the index of the prototype associated with its cluster\nPrototypeBased A cluster is a set of objects in which each object is closer ( more similar ) to the prototype that deﬁnes the cluster than to the prototype of any other cluster\nFor data with continuous attributes the prototype of a cluster is often a centroid ie the average ( mean ) of all the points in the clus ter\nFor many types of data the prototype can be regarded as the most central point and in such instances we commonly refer to prototype based clusters as centerbased clusters\nNot surprisingly such clusters tend GraphBased If the data is represented as a graph where the nodes are objects and the links represent connections among objects ( see Section ) then a cluster can be deﬁned as a connected component ie a group of objects that are connected to one another but that have no connection to objects outside the group\nAn important example of graphbased clusters are contiguitybased clusters where two objects are connected only if they are within a speciﬁed distance of each other\nOther types of graphbased clusters are also possible\nLike prototypebased clusters such clusters tend to be globular\nSome of these techniques have a natural interpretation in terms of graphbased clustering while others have an interpretation in terms of a prototypebased approach\n( b ) Centerbased clusters\n( c ) Contiguitybased clusters\nFor example many agglomerative hierarchical clustering techniques such as MIN MAX and Group Average come from a graphbased view of clusters\nIn the process we will also see some interesting relationships between prototype and graphbased clustering\n<eos>"
    },
    {
        "key": "file2_topic_58",
        "label": "documents, contain, contains, good, compres",
        "input": "<task:clean> <sos>\nCluster prototypes can also be used for data compres sion\nthan make a somewhat arbitrary assignment of the object to a single cluster it is placed in all of the equally good clusters\nInstead these approaches are most appropriate for avoiding the arbitrariness of assigning an object to only one cluster when it may be close to several\nHowever the sharedproperty approach also includes new types of clusters\nIdeally each cluster will contain documents from only one class\nIn reality each cluster contains documents from many classes\nNevertheless many clus ters contain documents primarily from just one class\nIn particular cluster which contains mostly documents from the Sports section is exceptionally good both in terms of purity and entropy\n<eos>"
    },
    {
        "key": "file2_topic_59",
        "label": "table, integer, concrete, metro, position",
        "input": "<task:clean> <sos>\nIn particular a table is created that consists of the prototypes for each cluster ie each prototype is assigned an integer value that is its position ( index ) in the table\nOften clusters and their cluster prototypes can be found much more eﬃciently\nIn many applications the notion of a cluster is not well deﬁned\nOften but not always the leaves of the tree are singleton clusters of individual data objects\nNot surprisingly there are several diﬀerent notions of a cluster that prove useful in practice\nTo give a concrete example we consider cluster and the Metro class of Table\n<eos>"
    },
    {
        "key": "file2_topic_60",
        "label": "neighbors, nearest, finding, prototypes, distance",
        "input": "<task:clean> <sos>\nEﬃciently Finding Nearest Neighbors\nFinding nearest neighbors can require computing the pairwise distance between all points\nIf objects are relatively close to the prototype of their cluster then we can use the prototypes to reduce the number of distance computations that are necessary to ﬁnd the nearest neighbors of an object\nIntuitively if two cluster prototypes are far apart then the objects in the corresponding clusters can not be nearest neighbors of each other\nConsequently to ﬁnd an objects nearest neighbors it is only necessary to compute the distance to objects in nearby clusters where the nearness of two clusters is measured by the distance between their prototypes\nThe basic approach is to look at the behavior of the distance from a point to its kth nearest neighbor which we will call the kdist\n<eos>"
    },
    {
        "key": "file2_topic_61",
        "label": "conceptual, unrelated, sharedproperty, property, deﬁne",
        "input": "<task:clean> <sos>\nThe goal is that the objects within a group be similar ( or related ) to one another and diﬀerent from ( or unrelated to ) the objects in other groups\nSharedProperty ( Conceptual Clusters ) More generally we can deﬁne a cluster as a set of objects that share some property\nThe process of ﬁnd ing such clusters is called conceptual clustering\n( e ) Conceptual clusters\n<eos>"
    },
    {
        "key": "file2_topic_62",
        "label": "links, singleton, start, bine, points",
        "input": "<task:clean> <sos>\nTo better which shows twenty points and three diﬀerent ways of dividing them into clusters\nDivisive Start with one allinclusive cluster and at each step split a cluster until only singleton clusters of individual points remain\nUsing graph terminology if you start with all points as singleton clusters and add links between points one at a time shortest links ﬁrst then these single links com bine the points into clusters\nConsider the points in group these points into three clusters then we should have no trouble ﬁnding these clusters since they are wellseparated\n<eos>"
    },
    {
        "key": "file2_topic_63",
        "label": "fuzzy, object, belongs, situations, weight",
        "input": "<task:clean> <sos>\nExclusive versus Overlapping versus Fuzzy The clusterings shown in There are many situations in which a point could reasonably be placed in more than one cluster and these situations are better addressed by nonexclusive clustering\nIn the most general sense an overlapping or nonexclusive clustering is used to reﬂect the fact that an object can simultaneously belong clustering is also often used when for example an object is between two or more clusters and could reasonably be assigned to any of these clusters\nIn a fuzzy clustering every object belongs to every cluster with a mem bership weight that is between ( absolutely doesnt belong ) and ( absolutely belongs )\nIn other words clusters are treated as fuzzy sets\n( Mathematically a fuzzy set is one in which an object belongs to any set with a weight that is between and\nIn fuzzy clustering we often impose the additional con straint that the sum of the weights for each object must equal )\nBe cause the membership weights or probabilities for any object sum to a fuzzy or probabilistic clustering does not address true multiclass situations such as the case of a student employee where an object belongs to multiple classes\nIn practice a fuzzy or probabilistic clustering is often converted to an exclusive clustering by assigning each object to the cluster in which its membership weight or probability is highest\n<eos>"
    },
    {
        "key": "file2_topic_64",
        "label": "measure, evalua, pected, ideas, correlations",
        "input": "<task:clean> <sos>\nSimilarly probabilistic clustering techniques compute the probability with which each Overview point belongs to each cluster and these probabilities must also sum to\nIn the following section we consider a cluster evalua tion measure that uses an approach based on these ideas to evaluate points clusters and the entire set of clusters\nAn overall measure of the goodness of a clustering can be obtained by computing the average silhouette coeﬃcient of all points\nTo illustrate this measure we calculated the correlation between the ideal and The correlations were and respectively which reﬂects the ex pected result that the clusters found by Kmeans in the random data are worse than the clusters found by Kmeans in data with wellseparated clusters\nThis measure is known as the Γ statistic in clustering validation literature\n<eos>"
    },
    {
        "key": "file2_topic_65",
        "label": "clique, completely, add, object, links",
        "input": "<task:clean> <sos>\nWellSeparated A cluster is a set of objects in which each object is closer ( or more similar ) to every other object in the cluster than to any object not in the cluster\nOne such approach ( Section ) deﬁnes a cluster as a clique ie a set of nodes in a graph that are completely connected to each other\nSpeciﬁcally if we add connections between objects in the order of their distance from one another a cluster is formed when a set of objects forms a clique\nUsing graph terminology if you start with all points as singleton clusters and add links between points one at a time shortest links ﬁrst then a group of points is not a cluster until all the points in it are completely linked ie form a clique\n<eos>"
    },
    {
        "key": "file2_topic_66",
        "label": "deﬁnitions, repre, suﬃciently, specify, mindsee",
        "input": "<task:clean> <sos>\nSometimes a threshold is used to specify that all the objects in a cluster must be suﬃciently close ( or similar ) to one another\nThis implies that each object in a contiguitybased cluster is closer to some other object in the cluster than to for twodimensional points\nThis deﬁnition encom passes all the previous deﬁnitions of a cluster eg objects in a centerbased cluster share the property that they are all closest to the same centroid or medoid\nThis requires deﬁning a notion of cluster proximity\nCluster proximity is typically deﬁned with a particular type of cluster in mindsee Section\nIf instead we take a prototypebased view in which each cluster is repre sented by a centroid diﬀerent deﬁnitions of cluster proximity are more natural\n<eos>"
    },
    {
        "key": "file2_topic_67",
        "label": "closer, center, point, derives, pure",
        "input": "<task:clean> <sos>\nThe distance between any two points in diﬀerent groups is larger than Cluster Analysis Basic Concepts and Algorithms the distance between any two points within a group\npoint is closer to all of the points in its cluster than to any point in another cluster\npoint is closer to the center of its cluster than to the center of any other cluster\nEach point is closer to at least one point in its cluster than to any point in another cluster\nPoints in a cluster share some general property that derives from the entire set of points\nEach smaller cluster is pure in the sense that it contains only points from one of the natural clusters\n<eos>"
    },
    {
        "key": "file2_topic_68",
        "label": "noise, contiguity, darkness, embarrassing, bridges",
        "input": "<task:clean> <sos>\nBy contrast a contiguity since the noise would tend to form bridges between clusters\nIf the value of k is too large then small clusters ( of size less than k ) are likely to be labeled as noise\nsity of the clusters and noise regions is indicated by their darkness\nWhile it is entertaining to ﬁnd patterns in clouds it is pointless and perhaps embarrassing to ﬁnd clusters in noise\n<eos>"
    },
    {
        "key": "file2_topic_69",
        "label": "density, region, radius, dense, centerbased",
        "input": "<task:clean> <sos>\nClus ters are regions of high density sep arated by regions of low density\nIn contrast points from every dense region are likely to be included unless the sample size is very small\nOther deﬁnitions In the centerbased approach density is estimated for a particular point in the data set by counting the number of points within a speciﬁed radius Eps of that point\nThis method is simple to implement but the density of any point will depend on the speciﬁed radius\nFor instance if the radius is large enough then all points will have a density of m the number of points in the data set\nLikewise if the radius is too small then all points will have a density of\nClassiﬁcation of Points According to CenterBased Density The centerbased approach to density allows us to classify a point as being ( ) in the interior of a dense region ( a core point ) ( ) on the edge of a dense region ( a border point ) or ( ) in a sparsely occupied region ( a noise or background noise points using a collection of twodimensional points\n<eos>"
    },
    {
        "key": "file2_topic_70_0",
        "label": "method, wards, squared, error, solution",
        "input": "<task:clean> <sos>\nThere are a number of such techniques but two of the most prominent are Kmeans and Kmedoid\nVarious techniques are used to ﬁx up the resulting clusters in order to produce a clustering that has lower SSE\nBoth methods are discussed in Section\nFamiliarity with optimization techniques especially those based on gradient descent may also be helpful\nOne technique which is known as gradient descent is based on picking an initial solution and then repeating the fol lowing two steps compute the change to the solution that best optimizes the objective function and then update the solution\nAn alternative technique Wards method also assumes that a cluster is represented by its centroid but it measures the proximity between two clusters in terms of the increase in the SSE that re Cluster Analysis Basic Concepts and Algorithms sults from merging the two clusters\nLike Kmeans Wards method attempts to minimize the sum of the squared distances of points from their cluster centroids\nMAX to the sample data set of six points\nWards Method and Centroid Methods For Wards method the proximity between two clusters is deﬁned as the in crease in the squared error that results when two clusters are merged\nWhile it may seem that this feature makes Wards method somewhat distinct from other hierarchical techniques it can be shown mathematically that Wards method is very similar to the group average method when the proximity be tween two points is taken to be the square of the distance between them\nWards method to the sample data set of six points\nThese techniques may seem similar to Kmeans but as we have remarked Wards method is the correct hierarchical analog\nWhile a general formula is appealing especially for implementation it is easier to understand the diﬀerent hierarchical methods by looking directly at the deﬁnition of clus ter proximity that each method uses\n<eos>"
    },
    {
        "key": "file2_topic_70_1",
        "label": "method, wards, squared, error, solution",
        "input": "<task:clean> <sos>\nCluster Analysis Basic Concepts and Algorithms For example although the minimize squared error criterion from Kmeans is used in deciding which clusters to merge in Wards method the clusters at each level do not represent local minima with respect to the total SSE\nNonetheless Wards method is often used as a robust method of initializing a Kmeans clustering indicating that a local minimize squared error objective function does have a connection to a global minimize squared error objective function\nThe following text provides a more precise description\nThe formal details are given in Algorithm\nOne possible approach to dealing with such issues is given in Section\n<eos>"
    },
    {
        "key": "file2_topic_71",
        "label": "wide, variety, enjoy, paragraphs, suggested",
        "input": "<task:clean> <sos>\nIn this section we will focus solely on Kmeans which is one of the oldest and most widely used clustering algorithms\nBut as suggested by the last few paragraphs Kmeans is a very general clustering algorithm and can be used with a wide variety of data types such as documents and time series\nStrengths and Weaknesses Kmeans is simple and can be used for a wide variety of data types\nAs with Kmeans these approaches are relatively old compared to many clustering algorithms but they still enjoy widespread use\n<eos>"
    },
    {
        "key": "file2_topic_72",
        "label": "updated, updates, zero, assigned, centroids",
        "input": "<task:clean> <sos>\nThe centroid of each cluster is then updated based on the points assigned to the cluster\nWe repeat the assignment and update steps until no point changes clusters or equivalently until the centroids remain the same\nuntil Centroids do not change\nAfter points are assigned to a centroid the centroid is then updated\nUpdating Centroids Incrementally Instead of updating cluster centroids after all points have been assigned to a cluster the centroids can be updated incrementally after each assignment of a point to a cluster\nNotice that this requires either zero or two updates to cluster centroids at each step since a point either moves to a new cluster ( two updates ) or stays in its current cluster ( zero updates )\n<eos>"
    },
    {
        "key": "file2_topic_73",
        "label": "choices, functions, assignmentupdate, combinations, state",
        "input": "<task:clean> <sos>\nThe operation of Kmeans ﬁnal clusters are found in four assignmentupdate steps\nFor some combinations of proximity functions and types of centroids K means always converges to a solution ie Kmeans reaches a state in which no points are shifting from one cluster to another and hence the centroids dont change\nCentroids and Objective Functions Step of the Kmeans algorithm was stated rather generally as recompute the centroid of each cluster since the centroid can vary depending on the proximity measure for the data and the goal of the clustering\nTotal Cohesion cosine ( x ci ) The General Case There are a number of choices for the proximity func tion centroid and objective function that can be used in the basic Kmeans K means choices including the two that we have just discussed\n<eos>"
    },
    {
        "key": "file2_topic_74",
        "label": "centroids, displaying, subﬁgure, reﬁne, tracted",
        "input": "<task:clean> <sos>\nIn these and other ﬁgures displaying Kmeans clustering each subﬁgure shows ( ) the centroids at the start of the iteration and ( ) the assignment of the points to those centroids\nIn the second step points are assigned to the updated centroids and the centroids Cluster Analysis Basic Concepts and Algorithms ( a ) Iteration\nK clusters are ex tracted from the hierarchical clustering and the centroids of those clusters are used as the initial centroids\nWe often reﬁne the resulting clusters by using their centroids as the initial centroids for the basic Kmeans algorithm\n<eos>"
    },
    {
        "key": "file2_topic_75",
        "label": "centroid, closest, consideration, hattan, marker",
        "input": "<task:clean> <sos>\nThe centroids are indicated by the symbol all points belonging to the same cluster have the same marker shape\nFor this example we use the mean as the centroid\nAssigning Points to the Closest Centroid To assign a point to the closest centroid we need a proximity measure that quantiﬁes the notion of closest for the speciﬁc data under consideration\nHowever the key point is this once we have speciﬁed a proximity measure and an objective function the centroid that we should choose can often be determined mathematically\nUsing the notation ci To illustrate the centroid of a cluster containing the three twodimensional points ( ) ( ) and ( ) is ( ( ) ( ( ) ) ( )\nNotice that for Man hattan ( L ) distance and the objective of minimizing the sum of the distances the appropriate centroid is the median of the points in a cluster\nWhen using centroids the cluster proximity is commonly deﬁned as the prox imity between cluster centroids\n<eos>"
    },
    {
        "key": "file2_topic_76",
        "label": "select, initial, farthest, centroids, selected",
        "input": "<task:clean> <sos>\nSelect K points as initial centroids\nThe following procedure is another approach to selecting initial centroids\nSelect the ﬁrst point at random or take the centroid of all points\nThen for each successive initial centroid select the point that is farthest from any of the initial centroids already selected\n( d ) Iteration centroids that is guaranteed to be not only randomly selected but also well separated\nOne approach is to choose the point that is farthest away from any current centroid\nAlthough this can be addressed by randomizing the order in which the points are processed the basic Kmeans approach of updating the centroids after all points have been assigned to clus ters has no order dependency\n<eos>"
    },
    {
        "key": "file2_topic_77",
        "label": "iteration, pressed, formally, equation, max",
        "input": "<task:clean> <sos>\n( b ) Iteration\nThis approach is ex pressed more formally in Algorithm\n( b ) MAX ( complete link )\nIn this equation p ( )\n<eos>"
    },
    {
        "key": "file2_topic_78_0",
        "label": "bisecting, kmeans, initialization, natural, algorithm",
        "input": "<task:clean> <sos>\nWhen the Kmeans algorithm terminates the natural groupings of points\nBisecting Kmeans ( described in Section ) is another approach that speeds up Kmeans by reducing the number of similarities computed\nGiven two diﬀerent sets of clusters that are produced by two diﬀerent runs of Kmeans we prefer the one with the smallest squared error since this means that the prototypes ( centroids ) of this clustering are a better representation of the points in their cluster\n( d ) Iteration is less susceptible to initialization problems ( bisecting Kmeans ) and using postprocessing to ﬁxup the set of clusters produced\nBisecting Kmeans The bisecting Kmeans algorithm is a straightforward extension of the basic Kmeans algorithm that is based on a simple idea to obtain K clusters split the set of all points into two clusters select one of these clusters to split and K means so on until K clusters have been produced\nThe details of bisecting Kmeans are given by Algorithm\nAlgorithm Bisecting Kmeans algorithm\nPerform several trial bisections of the chosen cluster\nfor i to number of trials do Bisect the selected cluster using basic Kmeans\nThis is necessary because although the Kmeans algorithm is guaranteed to ﬁnd a clustering that represents a local minimum with respect to the SSE in bisecting Kmeans we are using the K means algorithm locally ie to bisect individual clusters\nExample ( Bisecting Kmeans and Initialization )\nTo illustrate that bisecting Kmeans is less susceptible to initialization problems we show in In iteration two pairs of clusters are found in iteration the rightmost pair of clusters is split and in iteration the leftmost pair of clusters is split\nBisecting Kmeans has less trouble with initialization because it performs several trial bisections and takes the one with the lowest SSE and because there are only two centroids at each step\n<eos>"
    },
    {
        "key": "file2_topic_78_1",
        "label": "bisecting, kmeans, initialization, natural, algorithm",
        "input": "<task:clean> <sos>\nIn particular Kmeans has diﬃculty detecting the natural clusters when clusters have nonspherical shapes or widely diﬀerent Kmeans can not ﬁnd the three natural clusters because one of the clusters is much larger than the other two and hence the larger cluster is broken while one of the smaller clusters is combined with a portion of the larger cluster\nIn Kmeans ﬁnds two clusters that mix portions of the two natural clusters because the shape of the natural clusters is not globular\nThe diﬃculty in these three situations is that the Kmeans objective func tion is a mismatch for the kinds of clusters we are trying to ﬁnd since it is minimized by globular clusters of equal size and density or by clusters that are well separated\nSome variants including bisecting Kmeans are even more eﬃcient and are less suscepti ble to initialization problems\n<eos>"
    },
    {
        "key": "file2_topic_79",
        "label": "names, quantities, similarities, measures, modify",
        "input": "<task:clean> <sos>\nHowever there may be several types of proximity measures that are appropriate for a given type of data\n( If our proximities are distances then the names MIN and MAX are short and suggestive\nFor similarities however where higher values indicate closer points the names seem reversed\nThe proximity function can be a similarity a dissimilarity or a simple function of these quantities\nWe use distances but an analogous approach can be used for similarities\nThe ﬁrst compares an actual and idealized proximity matrix while the second uses visualization\nFor convenience we will refer to these two types of measures as classiﬁcationoriented and similarityoriented respectively\nPoint Point More generally we can use any of the measures for binary similarity that we saw in Section\nWe repeat the deﬁnitions of the four quantities used to deﬁne those similarity measures but modify our descriptive text to ﬁt the current context\n<eos>"
    },
    {
        "key": "file2_topic_80",
        "label": "bregman, divergence, divergences, properties, euclidean",
        "input": "<task:clean> <sos>\nFor example Manhattan ( L ) distance can be used for Euclidean data while the Jaccard measure is often employed for documents\nThe last entry in the table Bregman divergence ( Section ) is actually a class of proximity measures that includes the squared Euclidean distance L the Mahalanobis distance and cosine similarity\nThe importance of Bregman divergence functions is that any such function can be used as the basis of a K means style clustering algorithm with the mean as the centroid\nSpeciﬁcally if we use a Bregman divergence as our proximity function then the result ing clustering algorithm has the usual properties of Kmeans with respect to convergence local minima etc\nFurthermore the properties of such a cluster ing algorithm can be developed for all possible Bregman divergences\nIndeed Kmeans algorithms that use cosine similarity or squared Euclidean distance are particular instances of a general clustering algorithm based on Bregman divergences\n<eos>"
    },
    {
        "key": "file2_topic_81",
        "label": "values, sse, distribution, good, estimate",
        "input": "<task:clean> <sos>\nFor our objective function which measures the quality of a clustering we use the sum of the squared error ( SSE ) which is also known as scatter\nAs an example two Kmeans clusterings can be compared using either the SSE or entropy\nThe cluster eval uation measure I corresponds to traditional Kmeans and produces clusters that have good SSE values\nThe other measures produce clusters that are not as good with respect to SSE but that are more optimal with respect to the speciﬁed cluster validity measure\nWe generate many random sets of points having the same range as Cluster Analysis Basic Concepts and Algorithms Count the points in the three clusters ﬁnd three clusters in each data set using K means and accumulate the distribution of SSE values for these clusterings\nBy using this distribution of the SSE values we can then estimate the probability conservatively claim that there is less than a chance that a clustering such To conclude we stress that there is more to cluster evaluationsupervised or unsupervisedthan obtaining a numerical measure of cluster validity\n<eos>"
    },
    {
        "key": "file2_topic_82",
        "label": "assume, onedimensional, notation, simpliﬁes, notational",
        "input": "<task:clean> <sos>\nIn other words we calculate the error of each data point ie its Euclidean distance to the closest centroid and then compute the total sum of the squared errors\nWe assume that the data is onedimensional ie dist ( x y ) ( x y )\nThis does not change anything essential but greatly simpliﬁes the notation\nAgain for notational simplicity we use onedimensional data ie distL ci x\nTo simplify the notation we assume that the data is one dimensional ie dist ( x y ) ( xy )\n<eos>"
    },
    {
        "key": "file2_topic_83_0",
        "label": "average, group, proximity, min, dist",
        "input": "<task:clean> <sos>\nUsing the notation in Cluster Analysis Basic Concepts and Algorithms SSE dist ( ci x ) where dist is the standard Euclidean ( L ) distance between two objects in Euclidean space\nMIN deﬁnes cluster proximity as the prox imity between the closest two points that are in diﬀerent clusters or using graph terms the shortest edge between two nodes in diﬀerent subsets of nodes\nMAX takes the proximity between the farthest two points in diﬀerent clusters to be the cluster proximity or using graph terms the longest edge between two nodes in diﬀerent subsets of nodes\nAnother graphbased approach the group average technique deﬁnes cluster proximity to be the average pairwise proximities ( av illustrates these three approaches\n( a ) MIN ( single link )\n( c ) Group average\nAgglomerative Hierarchical Clustering Single Link or MIN For the single link or MIN version of hierarchical clustering the proximity of two clusters is deﬁned as the minimum of the distance ( maximum of the similarity ) between any two points in the two diﬀerent clusters\nAs another example the distance between clusters and is given by dist ( ) min ( dist ( ) dist ( ) dist ( ) dist ( ) ) min ( )\nComplete Link or MAX or CLIQUE For the complete link or MAX version of hierarchical clustering the proximity of two clusters is deﬁned as the maximum of the distance ( minimum of the similarity ) between any two points in the two diﬀerent clusters\nGroup Average For the group average version of hierarchical clustering the proximity of two clusters is deﬁned as the average pairwise proximity among all pairs of points in the diﬀerent clusters\nThus for group average the cluster proxim Cluster Analysis Basic Concepts and Algorithms ( a ) Group average clustering\n<eos>"
    },
    {
        "key": "file2_topic_83_1",
        "label": "average, group, proximity, min, dist",
        "input": "<task:clean> <sos>\nity proximity ( Ci Cj ) of clusters Ci and Cj which are of size mi and mj respectively is expressed by the following equation proximity ( Ci Cj ) proximity ( x y ) mi mj the group average approach to the sample data set of six points\nTo illustrate how group average works we calculate the distance between some clusters\nis a proximity function while mA mB and mQ are the number of points in clusters A B and Q respectively\n( This discussion applies only to cluster proximity schemes that involve sums such as centroid Wards and group average )\nWe will illustrate this using the group average technique discussed in Sec tion which is the unweighted version of the group average technique\nFor the weighted version of group averageknown as WPGMAthe coeﬃcients are constants αA αB β γ\nSimilarly the separation between two clusters can be measured where the centroid of a cluster is indicated by a\nE is a measure of separation deﬁned as the proximity of a cluster centroid to the overall centroid multiplied by the number of objects in the cluster\nFor instance for the SSE and points in Euclidean space it can be shown ( Equation ) that the average pairwise distance between Cluster SSE dist ( ci x ) dist ( x y ) ( ) Two Approaches to PrototypeBased Separation When proximity is measured by Euclidean distance the traditional measure of separation between clusters is the between group sum of squares ( SSB ) which is the sum of the squared distance of a cluster centroid ci to the overall mean c of all the data points\nFor the ith object calculate its average distance to all other objects in its cluster\nFor the ith object and any cluster not containing the object calculate the objects average distance to all the objects in the given cluster\n<eos>"
    },
    {
        "key": "file2_topic_84_0",
        "label": "sse, minimize, minimizes, objective, sae",
        "input": "<task:clean> <sos>\nGiven these assumptions it can be shown ( see Section ) that the centroid that minimizes the SSE of the cluster is the mean\nSteps and of the Kmeans algorithm directly attempt to minimize the SSE ( or more generally the objective function )\nStep forms clusters by assigning points to their nearest centroid which minimizes the SSE for the given set of centroids\nHowever the actions of Kmeans in Steps and are only guaranteed to ﬁnd a local minimum with respect to the SSE since they are based on optimizing the SSE for speciﬁc choices of the centroids and clusters rather than for all possible choices\nthat is only a local minimum\nAnother approach is to choose the replacement centroid from the cluster that has the highest SSE\nIn this way it is often possible to escape local SSE minima and still produce a clustering solution with the desired number of clusters\nYet another beneﬁt of incremental updates has to do with using objectives other than minimize SSE Suppose that we are given an arbitrary objective function to measure the goodness of a set of clusters\nSpeciﬁc examples of alternative objective functions are given in Section\nTherefore the ﬁnal set of clusters does not represent a clustering that is a local minimum with respect to the total SSE\nAs mentioned earlier given an objective function such as minimize SSE clustering can be treated as an optimization problem\nOne way to solve this problemto ﬁnd a global optimumis to enumerate all possible ways of di viding the points into clusters and then choose the set of clusters that best satisﬁes the objective function eg that minimizes the total SSE\nDerivation of Kmeans as an Algorithm to Minimize the SSE In this section we show how the centroid for the Kmeans algorithm can be mathematically derived when the proximity function is Euclidean distance and the objective is to minimize the SSE\nSpeciﬁcally we investigate how we can best update a cluster centroid so that the cluster SSE is minimized\n<eos>"
    },
    {
        "key": "file2_topic_84_1",
        "label": "sse, minimize, minimizes, objective, sae",
        "input": "<task:clean> <sos>\nIn mathematical terms we seek to minimize Equation which we repeat here specialized for onedimensional data\nSSE ( ci x ) Cluster Analysis Basic Concepts and Algorithms Here Ci is the ith cluster x is a point in Ci and ci is the mean of the ith We can solve for the kth centroid ck which minimizes Equation by diﬀerentiating the SSE setting it equal to and solving as indicated below\n( ci x ) ( ci x ) ( ck xk ) ( ck xk ) mkck xk ck Thus as previously indicated the best centroid for minimizing the SSE of a cluster is the mean of the points in the cluster\nDerivation of Kmeans for SAE To demonstrate that the Kmeans algorithm can be applied to a variety of diﬀerent objective functions we consider how to partition the data into K clusters such that the sum of the Manhattan ( L ) distances of points from the center of their clusters is minimized\nWe are seeking to minimize the sum of the L absolute errors ( SAE ) as given by the following equation where distL is the L distance\nSAE distL ( ci x ) We can solve for the kth centroid ck which minimizes Equation by diﬀerentiating the SAE setting it equal to and solving\n( It can be shown that the general clustering problem for an objective function such as minimize SSE is computationally infeasible )\nThis approach prevents a local optimization criterion from becoming a global optimization criterion\nNote that Equation is the cluster SSE if we let proximity be the squared Euclidean distance\n<eos>"
    },
    {
        "key": "file2_topic_85_0",
        "label": "cohesion, separation, prototypebased, graphbased, view",
        "input": "<task:clean> <sos>\nHere we assume that the document data is represented as a documentterm of the documents in a cluster to the cluster centroid this quantity is known as the cohesion of the cluster\nrelationships and the order in which the clusters were merged ( agglomerative view ) or split ( divisive view )\nGraphBased View of Cohesion and Separation For graphbased clusters the cohesion of a cluster can be deﬁned as the sum of the weights of the links in the proximity graph that connect points within the as nodes a link between each pair of data objects and a weight assigned to each link that is the proximity between the two data objects connected by the link )\nLikewise the separation between two clusters can be measured by the sum of the weights of the links from points in one cluster to points in the other Mathematically cohesion and separation for a graphbased cluster can be expressed using Equations and respectively\ncohesion ( Ci ) proximity ( x y ) separation ( Ci Cj ) proximity ( x y ) ( ) Cluster Evaluation ( a ) Cohesion\nPrototypeBased View of Cohesion and Separation For prototypebased clusters the cohesion of a cluster can be deﬁned as the sum of the proximities with respect to the prototype ( centroid or medoid ) of the cluster\nCohesion for a prototypebased cluster is given in Equation while two measures for separation are given in Equations and respec tively where ci is the prototype ( centroid ) of cluster Ci and c is the overall prototype ( centroid )\nThere are two measures for separation because as we will see shortly the separation of cluster prototypes from an overall prototype is sometimes directly related to the separation of cluster prototypes from one another\nI is a measure of cohesion in terms of the pairwise proximity of objects in the cluster divided by the cluster size\nI is a measure of cohesion based on the sum of the proximities of objects in the cluster to the cluster centroid\n<eos>"
    },
    {
        "key": "file2_topic_85_1",
        "label": "cohesion, separation, prototypebased, graphbased, view",
        "input": "<task:clean> <sos>\nG which is a measure based on both cohesion and separation is the sum of the pairwise proximity of all objects in the cluster with all objects outside the clusterthe total weight of the edges of the proximity graph that must be cut to separate the cluster from all other clustersdivided by the sum of the pairwise proximity of objects in the cluster\nRelationship between PrototypeBased Cohesion and GraphBased Cohesion While the graphbased and prototypebased approaches to measuring the co hesion and separation of a cluster seem distinct for some proximity measures they are equivalent\nIn particular if the cluster sizes are Cluster Analysis Basic Concepts and Algorithms equal ie mi mK then this relationship takes the simple form given by motivates the deﬁnition of prototype separation in terms of both Equations and\nA cluster that has a high value of cohesion may be considered better than a cluster that has a lower value\nIf for example a cluster is not very cohesive then we may want to split it into several subclus ters\nOn the other hand if two clusters are relatively cohesive but not well separated we may want to merge them into a single cluster\nWe can also evaluate the objects within a cluster in terms of their con tribution to the overall cohesion or separation of the cluster\nObjects that contribute more to the cohesion and separation are near the interior of the cluster\n<eos>"
    },
    {
        "key": "file2_topic_86",
        "label": "cohesion, separation, relationship, ssb, total",
        "input": "<task:clean> <sos>\nThe analogous quantity to the total SSE is the total cohesion which is given by Equation\nThe validity function can be cohesion separation or some combination of these quantities\nIf it is separation then lower values are better\nTotal SSB K dist ( ci cj ) ( ) Relationship between Cohesion and Separation In some cases there is also a strong relationship between cohesion and separa tion\nThe importance of this result is that minimizing SSE ( cohesion ) is equivalent to maximizing SSB ( separation )\n<eos>"
    },
    {
        "key": "file2_topic_87",
        "label": "initial, randomly, centroids, poor, runs",
        "input": "<task:clean> <sos>\nChoosing Initial Centroids When random initialization of centroids is used diﬀerent runs of Kmeans typically produce diﬀerent total SSEs\nA common approach is to choose the initial centroids randomly but the resulting clusters are often poor\nExample ( Poor Initial Centroids )\nRandomly selected initial cen troids may be poor\nWe provide an example of this using the same data set sult from two particular choices of initial centroids\nExample ( Limits of Random Initialization )\nOne technique that is commonly used to address the problem of choosing initial centroids is to perform multiple runs each with a diﬀerent set of randomly chosen initial centroids and then select the set of clusters with the minimum SSE\nBecause of the problems with using randomly selected initial centroids which even repeated runs may not overcome other techniques are often em ployed for initialization\n<eos>"
    },
    {
        "key": "file2_topic_88",
        "label": "outliers, eliminate, pression, proﬁtable, unduly",
        "input": "<task:clean> <sos>\nUnfortunately such an approach can select outliers rather than points in dense regions ( clusters )\nSince outliers are rare they tend not to show up in a random sample\nOutliers When the squared error criterion is used outliers can unduly inﬂuence the clusters that are found\nIn particular when outliers are present the resulting cluster centroids ( prototypes ) may not be as representative as they otherwise would be and thus the SSE will be higher as well\nBecause of this it is often useful to discover outliers and eliminate them beforehand\nIt is important however to appreciate that there are certain clustering applications for which outliers should not be eliminated\nWhen clustering is used for data com pression every point must be clustered and in some cases such as ﬁnancial analysis apparent outliers eg unusually proﬁtable customers can be the most interesting points\nAn obvious issue is how to identify outliers\nA number of techniques for remove outliers before clustering we avoid clustering points that will not clus ter well\nAlternatively outliers can also be identiﬁed in a postprocessing step\nAlso we may want to eliminate small clusters since they frequently represent groups of outliers\nKmeans also has trouble clustering data that contains outliers\nOutlier detection and removal can help signiﬁcantly in such situations\nThe median of a group of points is straightforward to compute and less susceptible to distortion by outliers\n<eos>"
    },
    {
        "key": "file2_topic_89",
        "label": "approximation, tolerate, sample, reduced, willing",
        "input": "<task:clean> <sos>\nTo overcome these problems this approach is often applied to a sample of the points\nAlso the computation involved in ﬁnding the initial centroids is greatly reduced because the sample size is typically much smaller than the number of points\nNonetheless many approaches have been developed most of them for points in lowdimensional Euclidean space\nIf for example we are clustering for utility we may be willing to tolerate only a certain level of error in the approximation of our points by a cluster centroid\n<eos>"
    },
    {
        "key": "file2_topic_90",
        "label": "complexity, time, required, matrix, assuming",
        "input": "<task:clean> <sos>\nIn particular the time required is O ( I K mn ) where I is the number of iterations required for convergence\nCompute the proximity matrix if necessary\nThis requires the storage of m proximities ( assuming the proximity matrix is symmetric ) where m is the number of data points\nHence the total space complexity is O ( m )\nO ( m ) time is required to compute the proximity matrix\nWithout modiﬁcation this would yield a time complexity of O ( m )\nOf course the time complexity of O ( m log m ) and the space complexity of O ( m ) are prohibitive in many cases\nIn the worst case this complexity is O ( m )\nThis approach may seem hopelessly expensive for large data sets since the computation of the proximity matrix takes O ( m ) time where m is the number of objects but with sampling this method can still be used\n<eos>"
    },
    {
        "key": "file2_topic_91",
        "label": "sse, total, split, largest, reduce",
        "input": "<task:clean> <sos>\nThis will typically split the cluster and reduce the overall SSE of the clustering\nK means Reducing the SSE with Postprocessing An obvious way to reduce the SSE is to ﬁnd more clusters ie to use a larger K However in many cases we would like to improve the SSE but dont want to increase the number of clusters\nThe strategy is to focus on individual clusters since the total SSE is simply the sum of the SSE contributed by each cluster\n( We will use the terminology total SSE and cluster SSE respectively to avoid any potential confusion )\nWe can change the total SSE by performing various operations on the clusters such as splitting or merging clusters\nTwo strategies that decrease the total SSE by increasing the number of clusters are the following Split a cluster The cluster with the largest SSE is usually chosen but we could also split the cluster with the largest standard deviation for one particular attribute\nIde ally the cluster that is dispersed should be the one that increases the total SSE the least\n<eos>"
    },
    {
        "key": "file2_topic_92",
        "label": "list, repeat, initialize, excluding, reassigned",
        "input": "<task:clean> <sos>\nIf there are several empty clusters then this process can be repeated several times\nUsing an incremental update strategy guarantees that empty clusters are not produced since all clusters start with a single point and if a cluster ever has only one point then that point will always be reassigned to the same cluster\nInitialize the list of clusters to contain the cluster consisting of all points\nrepeat Remove a cluster from the list of clusters\nuntil Until the list of clusters contains K clusters\nrepeat Merge the closest two clusters\nuntil Only one cluster remains\nThe space needed to keep track of the clusters is proportional to the number of clusters which is m excluding singleton clusters\nnique puts the objects in the same cluster for the ﬁrst time\n<eos>"
    },
    {
        "key": "file2_topic_93",
        "label": "dist, splitting, merged, stage, decision",
        "input": "<task:clean> <sos>\nOne commonly used approach is to use alternate cluster splitting and merging phases\nThere are a number of diﬀerent ways to choose which cluster to split\nIn this case we need to decide which cluster to split at each step and how to do the splitting\nAfter that step there are m iterations involving steps and because there are m clusters at the start and two clusters are merged during each iteration\ndist ( ) ( ) ( ) dist ( ) ( ) ( ) dist ( ) ( ) ( ) Because dist ( ) is smaller than dist ( ) and dist ( ) clusters and are merged at the fourth stage\nHowever once a decision is made to merge two clusters it can not be undone at a later time\n<eos>"
    },
    {
        "key": "file2_topic_94",
        "label": "choose, largest, increase, sse, merge",
        "input": "<task:clean> <sos>\nAnother approach is to choose randomly from all points or from the points with the highest Two strategies that decrease the number of clusters while trying to mini mize the increase in total SSE are the following Disperse a cluster This is accomplished by removing the centroid that cor responds to the cluster and reassigning the points to other clusters\nMerge two clusters The clusters with the closest centroids are typically chosen although another perhaps better approach is to merge the two clusters that result in the smallest increase in total SSE\nend for Select the two clusters from the bisection with the lowest total SSE\nWe can choose the largest cluster at each step choose the one with the largest SSE or use a criterion based on both size and SSE\nFind the minimum such value with respect to all clusters call this value bi\n<eos>"
    },
    {
        "key": "file2_topic_95",
        "label": "updating, incremental, weight, added, der",
        "input": "<task:clean> <sos>\nIn addition if incremental updating is used the relative weight of the point being added may be adjusted eg the weight of points is often decreased as the clustering proceeds\nThese update issues are similar to those involved in updating weights for artiﬁcial neural networks\nOn the negative side updating centroids incrementally introduces an or der dependency\nAlso incremental updates are slightly more expensive\n<eos>"
    },
    {
        "key": "file2_topic_96",
        "label": "weights, unweighted, size, account, gives",
        "input": "<task:clean> <sos>\nWhile this can result in better accuracy and faster convergence it can be diﬃcult to make a good choice for the relative weight especially in a wide variety of situations\nThere are two approaches weighted which treats all clusters equally and unweighted which takes the number of points in each cluster into account\nNote that the terminology of weighted or unweighted refers to the data points not the clusters\nIn other words treating clusters of unequal size equally gives diﬀerent weights to the points in diﬀerent clusters while taking the cluster size into account gives points in diﬀerent clusters the same weight\nIn general unweighted approaches are preferred unless there is reason to be lieve that individual points should have diﬀerent weights eg perhaps classes of objects have been unevenly sampled\nThe weights will vary depending on the cluster validity measure\nIn some cases the weights are simply or the size of the cluster while in other cases they reﬂect a more complicated property such as the square root of the are better\nHowever we need to decide what weights to use\nNot sur prisingly the weights used can vary widely although typically they are some measure of cluster size\n<eos>"
    },
    {
        "key": "file2_topic_97",
        "label": "value, signiﬁcant, need, chance, diﬀerence",
        "input": "<task:clean> <sos>\nIt is also quite eﬃcient even though multiple runs are often performed\nSecond we need a framework to interpret any measure\nFinally if a measure is too complicated to apply or to understand then few will use it\nTo give an example the diﬀerent trials\nIndeed they typically give us a single number as a measure of that goodness\nHowever we are then faced with the problem of interpreting the signiﬁcance of this number a task that may be even more diﬃcult\nSometimes however there may not be a minimum or maximum value or the scale of the data may aﬀect the interpretation\nAlso even if there are minimum and maximum values with obvious interpretations intermediate values still need to be interpreted\nIn some cases we can use an absolute standard\nSpeciﬁcally we attempt to judge how likely it is that our observed value may be achieved by random chance\nThe value is good if it is unusual ie if it is unlikely to be the result of random chance\nSuppose that we want a measure of data\nUn less this value has a natural interpretation based on the deﬁnition of the mea sure we need to interpret this value in some way\nAlthough one value will almost always be better than another it can be diﬃcult to determine if the diﬀerence is signiﬁcant\nNote that there are two aspects to this signiﬁcance whether the diﬀerence is statistically signiﬁcant ( repeatable )\n<eos>"
    },
    {
        "key": "file2_topic_98",
        "label": "large, good, improved, partitioned, oversample",
        "input": "<task:clean> <sos>\nIt can not handle nonglobular clusters or clusters of diﬀerent sizes and densities although it can typically ﬁnd pure subclusters if a large enough number of clusters is speciﬁed\nConsequently this is not a good measure for many density or contiguitybased clusters because they are not globular and may be closely intertwined with other clusters\nIt may be necessary to oversample small clusters and undersample large ones to obtain an adequate representation of all clusters\nThe purity and entropy of the other clusters is not as good but can typically be greatly improved if the data is partitioned into a larger number of clusters\n<eos>"
    },
    {
        "key": "file2_topic_99",
        "label": "link, complete, single, technique, produced",
        "input": "<task:clean> <sos>\nThese points were clustered using the singlelink technique that is described in Section\nFor that reason we usually prefer to use the alternative names single link and com plete link respectively )\nThe single link technique is good at handling nonelliptical shapes but is sensitive to noise and outliers\nComplete link is less susceptible to noise and outliers but it can break large clusters and it favors globular shapes\nAs with single link points and Agglomerative Hierarchical Clustering ( a ) Complete link clustering\nThis is an intermediate approach between the single and complete link approaches\nThe clustering that is produced is diﬀerent from those produced by single link complete link and group average\nThis includes the point itself\nThis technique is graphically point A is including A itself\nWe calculated the link technique seems to ﬁt the data less well than the clusterings produced by complete link group average and Wards method\nThe same experiment was performed for the wellseparated points of\n<eos>"
    },
    {
        "key": "file2_topic_100",
        "label": "matrix, update, proximity, requires, cost",
        "input": "<task:clean> <sos>\nUpdate the proximity matrix to reﬂect the proximity between the new cluster and the original clusters\nIf performed as a linear search of the proximity matrix then for the ith iteration step requires O ( ( mi ) ) time which is proportional to the current number of clusters squared\nStep only requires O ( m i ) time to update the proximity matrix after the merger of two clusters\nIf the distances from each cluster to all other clusters are stored as a sorted list ( or heap ) it is possible to reduce the cost of ﬁnding the two closest clusters to O ( m i )\nIn stead the proximity matrix is updated as clustering occurs\n<eos>"
    },
    {
        "key": "file2_topic_101",
        "label": "core, point, border, eps, minpts",
        "input": "<task:clean> <sos>\nCore points These points are in the interior of a densitybased cluster\nA point is a core point if the number of points within a given neighborhood around the point as determined by the distance function and a user speciﬁed distance parameter Eps exceeds a certain threshold MinPts core point for the indicated radius ( Eps ) if MinPts\nBorder points A border point is not a core point but falls within the neigh border point can fall within the neighborhoods of several core points\nPut an edge between all core points that are within Eps of each other\n<eos>"
    },
    {
        "key": "file2_topic_102",
        "label": "core, close, border, point, enoughwithin",
        "input": "<task:clean> <sos>\nAny two core points that are close enoughwithin a distance Eps of one anotherare put in the same cluster\nLikewise any border point that is close enough to a core point is put in the same cluster as the core point\n( Ties may need to be resolved if a border point is close to core points from diﬀerent clusters )\nMake each group of connected core points into a separate cluster\nAssign each border point to one of the clusters of its associated core points\n<eos>"
    },
    {
        "key": "file2_topic_103",
        "label": "time, complexity, highdimensional, dbscan, identiﬁcation",
        "input": "<task:clean> <sos>\nTime and Space Complexity The basic time complexity of the DBSCAN algorithm is O ( m time to ﬁnd points in the Epsneighborhood ) where m is the number of points\nHowever in lowdimensional spaces there are data structures such as kdtrees that allow eﬃcient retrieval of all DBSCAN points within a given distance of a speciﬁed point and the time complexity can be as low as O ( m log m )\nThe space requirement of DBSCAN even for highdimensional data is O ( m ) because it is only necessary to keep a small amount of data for each point ie the cluster label and the identiﬁcation of each point as a core border or noise point\nFinally DBSCAN can be expensive when the computation of nearest neighbors requires computing all pairwise proximities as is usually the case for highdimensional data\n<eos>"
    },
    {
        "key": "file2_topic_104",
        "label": "labels, externally, classiﬁcation, results, predicted",
        "input": "<task:clean> <sos>\nCluster Evaluation In supervised classiﬁcation the evaluation of the resulting classiﬁcation model is an integral part of the process of developing a classiﬁcation model and there are wellaccepted evaluation measures and procedures eg accuracy and crossvalidation respectively\nEvaluating how well the results of a cluster analysis ﬁt the data without reference to external information\nComparing the results of a cluster analysis to externally known results such as externally provided class labels\nSupervised Measures of Cluster Validity When we have external information about data it is typically in the form of externally derived class labels for the data objects\nIn such cases the usual procedure is to measure the degree of correspondence between the cluster labels and the class labels\nIn the case of classiﬁcation we measure the degree to which predicted class labels correspond to actual class labels but for the measures just mentioned nothing fundamental is changed by using cluster labels in stead of predicted class labels\n<eos>"
    },
    {
        "key": "file2_topic_105",
        "label": "evaluation, validation, nonetheless, important, toolkit",
        "input": "<task:clean> <sos>\nHowever because of its very nature cluster evaluation is not a welldeveloped or commonly used part of cluster analysis\nNonetheless cluster evaluation or cluster validation as it is more tradition ally called is important and this section will review some of the most common and easily applied approaches\nThere might be some confusion as to why cluster evaluation is necessary\nNonetheless cluster evaluation should be a part of any cluster analysis\nHowever the clusters do not look compelling for any of Cluster Evaluation the three methods\nThe following is a list of several important issues for cluster validation\nThe CLUstering TOolkit ( CLUTO ) ( see the bibliographic notes ) uses the cluster measures not mentioned here to drive the clustering process\n<eos>"
    },
    {
        "key": "file2_topic_106",
        "label": "interpret, supposed, unnecessarily, informal, exam",
        "input": "<task:clean> <sos>\nHence evaluation seems like an unnecessarily complicated addition to what is supposed to be an informal process\nWe conclude this section with a short discussion of how to interpret the values of ( unsupervised or supervised ) validity measures\nWe present an exam ple to illustrate these measures\nA common approach is to interpret the value of our validity measure in statistical terms\n<eos>"
    },
    {
        "key": "file2_topic_107",
        "label": "validity, measures, evaluate, measure, type",
        "input": "<task:clean> <sos>\nFurthermore since there are a number of diﬀerent types of clustersin some sense each clustering algorithm deﬁnes its own type of clusterit may seem that each situation might require a diﬀerent evaluation measure\nA further distinction can be made with respect to items and Do we want to evaluate the entire clustering or just individual clusters\nWhile it is possible to develop various numerical measures to assess the diﬀerent aspects of cluster validity mentioned above there are a number of challenges\nFirst a measure of cluster validity may be quite limited in the scope of its applicability\nThe evaluation measures or indices that are applied to judge various aspects of cluster validity are traditionally classiﬁed into the following three types\nUnsu pervised measures of cluster validity are often further divided into two classes measures of cluster cohesion ( compactness tightness ) which determine how closely related the objects in a cluster are and measures of cluster separation ( isolation ) which determine how distinct or well separated a cluster is from other clusters\nIn general we can consider expressing overall cluster validity for a set of K clusters as a weighted sum of the validity of individual clusters overall validity wi validity ( Ci )\nMany of these measures of cluster validity also can be used to evaluate individual clusters and objects\nOne of the most common uses of this measure is to evaluate which type of hierarchical clustering is best for a particular type of data\nAssessing the Signiﬁcance of Cluster Validity Measures Cluster validity measures are intended to help us measure the goodness of the clusters that we have obtained\nThe minimum and maximum values of cluster evaluation measures may provide some guidance in many cases\nIf our cluster evaluation measure is deﬁned such that lower values indicate stronger clusters then we can use statistics to evaluate whether the value we have obtained is unusually low provided we have a distribution for the evaluation measure\n<eos>"
    },
    {
        "key": "file2_topic_108",
        "label": "tendency, try, evaluate, quality, resulting",
        "input": "<task:clean> <sos>\nDetermining the clustering tendency of a set of data ie distinguish ing whether nonrandom structure actually exists in the data\nClustering Tendency One obvious way to determine if a data set has clusters is to try to cluster it\nTo address this issue we could evaluate the resulting clusters and only claim that a data set has clusters if at least some of the clusters are of good quality\nTo handle this additional problem we could use multiple algorithms and again evaluate the quality of the resulting clusters\nIf the clusters are uniformly poor then this may indeed indicate that there are no clusters in the data\nAlternatively and this is the focus of measures of clustering tendency we can try to evaluate whether a data set has clusters without clustering\nThe most common approach especially for data in Euclidean space has been to use statistical tests for spatial randomness\n<eos>"
    },
    {
        "key": "file2_topic_109",
        "label": "having, pairs, class, match, association",
        "input": "<task:clean> <sos>\nIf we obtain a value of for a measure that evaluates how well cluster labels match externally provided class labels does this value represent a good fair or poor match\nThe goodness of a match often can be measured by looking at the statistical distribution of this value ie how likely it is that such a value occurs by chance\nThe second group of methods is related to the similarity measures approaches measure the extent to which two objects that are in the same class are in the same cluster and vice versa\nSimilarityOriented Measures of Cluster Validity The measures that we discuss in this section are all based on the premise that any two objects that are in the same cluster should be in the same class and vice versa\n( There are m ( m ) such pairs if m is the number of objects )\nf number of pairs of objects having a diﬀerent class and a diﬀerent cluster f number of pairs of objects having a diﬀerent class and the same cluster f number of pairs of objects having the same class and a diﬀerent cluster f number of pairs of objects having the same class and the same cluster In particular the simple matching coeﬃcient which is known as the Rand statistic in this context and the Jaccard coeﬃcient are two of the most fre quently used cluster validity measures\nSame Cluster Diﬀerent Cluster Same Class Diﬀerent Class Previously in the context of association analysissee Section we presented an extensive discussion of measures of association that can be used measures can also be applied to cluster validity\n<eos>"
    },
    {
        "key": "file2_topic_110",
        "label": "entropy, pij, calculated, purity, entropies",
        "input": "<task:clean> <sos>\nWe then consider supervised approaches to cluster validity such as entropy purity and the Jaccard measure\nThe ﬁrst set of techniques use measures from classiﬁcation such as entropy purity and the Fmeasure\nNext we quickly review the deﬁnitions of these Entropy The degree to which each cluster consists of objects of a single class\nUsing this class distribution the entropy of each cluster i is calculated using the standard formula ei L j pij log pij where L is the number of classes\nThe total entropy for a set of clusters is calculated as the sum of the entropies of each cluster weighted by the size of each cluster ie e K m ei where K is the number of clusters and m is the total number of data points\n<eos>"
    },
    {
        "key": "file2_topic_111",
        "label": "ssb, total, mean, squares, sum",
        "input": "<task:clean> <sos>\nBy summing the SSB over all clusters we obtain the total SSB which is given by Equation where ci is the mean of the ith cluster and c is the overall mean\nThe higher the total SSB of a clustering the more separated the clusters are from one another\nTotal SSB mi dist ( ci c ) ( ) It is straightforward to show that the total SSB is directly related to the pairwise distances between the centroids\nSpeciﬁcally it is possible to show that the sum of the total SSE and the total SSB is a constant ie that it is equal to the total sum of squares ( TSS ) which is the sum of squares of the distance of each point to the overall mean of the data\n<eos>"
    },
    {
        "key": "file2_topic_112",
        "label": "silhouette, coeﬃcient, coeﬃcients, average, distinct",
        "input": "<task:clean> <sos>\nThe Silhouette Coeﬃcient The popular method of silhouette coeﬃcients combines both cohesion and sep aration\nThe following steps explain how to compute the silhouette coeﬃcient for an individual point a process that consists of the following three steps\nFor the ith object the silhouette coeﬃcient is si ( bi ai ) max ( ai bi )\nThe value of the silhouette coeﬃcient can vary between and negative value is undesirable because this corresponds to a case in which ai the average distance to points in the cluster is greater than bi the minimum average distance to points in another cluster\nWe want the silhouette coeﬃcient to be positive ( ai bi ) and for ai to be as close to as possible since the coeﬃcient assumes its maximum value of when ai\nCluster Analysis Basic Concepts and Algorithms Silhouette Coefficient We can compute the average silhouette coeﬃcient of a cluster by simply taking the average of the silhouette coeﬃcients of points belonging to the cluster\nsilhouette coeﬃcients for points in clusters\nDarker shades indicate lower silhouette coeﬃcients\nshows the average silhouette coeﬃcient versus the number of clusters for the same data\nThere is a distinct knee in the SSE and a distinct peak in the silhouette coeﬃcient when the number of clusters is equal to\nThere is a knee that indicates this in the SSE curve but the silhouette coeﬃcient curve is not Cluster Evaluation Number of Clusters Number of Clusters Silhouette Coefficient as clear\nSpeciﬁcally we need to compute the following four quantities for all pairs of distinct objects\n<eos>"
    },
    {
        "key": "file2_topic_113",
        "label": "matrix, similarity, ijth, ideal, labels",
        "input": "<task:clean> <sos>\nMeasuring Cluster Validity via Correlation If we are given the similarity matrix for a data set and the cluster labels from a cluster analysis of the data set then we can evaluate the goodness of the clustering by looking at the correlation between the similarity matrix and an ideal version of the similarity matrix based on the cluster labels\nJudging a Clustering Visually by Its Similarity Matrix The previous technique suggests a more general qualitative approach to judg ing a set of clusters Order the similarity matrix with respect to cluster labels and then plot it\nIn theory if we have wellseparated clusters then the simi larity matrix should be roughly blockdiagonal\nIf not then the patterns dis played in the similarity matrix can reveal the relationships between clusters\nWe can take a sample of data points from each cluster compute the similarity between these points and plot the result\nWe can view this approach to cluster validity as involving the comparison of two matrices ( ) the ideal cluster similarity matrix discussed previously which has a in the ijth entry if two objects i and j are in the same cluster and otherwise and ( ) an ideal class similarity matrix deﬁned with respect to class labels which has a in the ijth entry if Cluster Evaluation two objects i and j belong to the same class and a otherwise\nAs before we can take the correlation of these two matrices as the measure of cluster validity\n<eos>"
    },
    {
        "key": "file2_topic_114",
        "label": "matrices, similarity, rows, diagonal, simplicity",
        "input": "<task:clean> <sos>\n( With minor changes the following applies to proximity matrices but for simplicity we discuss only similarity matrices )\nThus if we sort the rows and columns of the similarity matrix so that all objects belonging to the same class are together then an ideal similarity matrix has a block diagonal structure\nAgain all of this can be applied to dissimilarity matrices but for simplicity we will only discuss similarity matrices\nExample ( Visualizing a Similarity Matrix )\ndiagonal pattern in the reordered similarity matrix\n( For example we can convert these two matrices into binary vectors by appending the rows )\n<eos>"
    },
    {
        "key": "file2_topic_115",
        "label": "similarity, matrix, ideal, sorted, indicates",
        "input": "<task:clean> <sos>\nMore speciﬁcally an ideal cluster is one whose points have a similarity of to all points in the cluster and a similarity of to all points in other clusters\nIn other words the similarity is nonzero ie inside the blocks of the similarity Cluster Evaluation matrix whose entries represent intracluster similarity and elsewhere\nThe ideal similarity matrix is constructed by creating a matrix that has one row and one column for each data pointjust like an actual similarity matrix and assigning a to an entry if the associated pair of points belongs to the same cluster\nHigh correlation between the ideal and actual similarity matrices indicates that the points that belong to the same cluster are close to each other while low correlation indicates the opposite\nPoints Points Similarity ( b ) Similarity matrix sorted by Kmeans cluster labels\nPoints Points Similarity ( b ) Similarity matrix sorted Kmeans cluster labels\nPoints Points Similarity ( c ) Similarity matrix sorted by complete link cluster labels\nThe ideal cluster and class similarity matrices are given in Tables and\n<eos>"
    },
    {
        "key": "file2_topic_116",
        "label": "correlation, matrices, entries, actual, ideal",
        "input": "<task:clean> <sos>\n( Since the actual and ideal similarity matrices are symmetric the correlation is calculated only among the n ( n ) entries below or above the diagonal of the matrices )\nExample ( Correlation of Actual and Ideal Similarity Matrices )\nExample ( Cophenetic Correlation Coeﬃcient )\nExample ( Correlation between Cluster and Class Matrices )\nThe correlation between the entries of these two matrices is\n<eos>"
    },
    {
        "key": "file2_topic_117",
        "label": "preexisting, hierarchical, clusterings, supervised, partitional",
        "input": "<task:clean> <sos>\nUnsupervised Evaluation of Hierarchical Clustering The previous approaches to cluster evaluation are intended for partitional clusterings\nCluster Validity for Hierarchical Clusterings So far in this section we have discussed supervised measures of cluster va lidity only for partitional clusterings\nSupervised evaluation of a hierarchical clustering is more diﬃcult for a variety of reasons including the fact that a preexisting hierarchical structure often does not exist\nHere we will give an example of an approach for evaluating a hierarchical clustering in terms of a ( ﬂat ) set of class labels which are more likely to be available than a preexisting hierarchical structure\n<eos>"
    },
    {
        "key": "file2_topic_118",
        "label": "statistic, rand, jaccard, columns, hopkins",
        "input": "<task:clean> <sos>\nExample ( Hopkins Statistic )\nThe Hopkins statistic H is then deﬁned by Equation i wi i ui p i wi ( ) If the randomly generated points and the sample of data points have roughly the same nearest neighbor distances then H will be near\nValues of H near and indicate respectively data that is highly clustered and data that is regularly distributed in the data space\nThe average value of H was with a standard deviation of\nThe ﬁrst column indicates the clus ter while the next six columns together form the confusion matrix ie these columns indicate how the documents of each category are distributed among the clusters\nThe last two columns are the entropy and purity of each cluster respectively\nRand statistic f f f f f f ( ) Cluster Analysis Basic Concepts and Algorithms Jaccard coeﬃcient f f f ( ) Example ( Rand and Jaccard Measures )\nBased on these formulas we can readily compute the Rand statistic and Jaccard coeﬃcient for the example based on Tables and\nNoting that f f f and f the Rand statistic ( ) and the Jaccard coeﬃcient ( )\n<eos>"
    },
    {
        "key": "file2_topic_119",
        "label": "precision, recall, class, purity, pij",
        "input": "<task:clean> <sos>\nThese measures evaluate the extent to which a cluster contains objects of a single class\nFor each cluster the class distribution of the data is calculated ﬁrst ie for cluster j we compute pij the probability that a member of cluster i belongs to class j as pij mijmi where mi is the number of objects in cluster i and mij is the number of objects of class j in cluster i\nPurity Another measure of the extent to which a cluster contains objects of a single class\nUsing the previous terminology the purity of cluster i is pi max pij the overall purity of a clustering is purity K m pi\nPrecision The fraction of a cluster that consists of objects of a speciﬁed class\nThe precision of cluster i with respect to class j is precision ( i j ) pij\nRecall The extent to which a cluster contains all objects of a speciﬁed class\nThe recall of cluster i with respect to class j is recall ( i j ) mijmj where mj is the number of objects in class j Fmeasure A combination of both precision and recall that measures the extent to which a cluster contains only objects of a particular class and all objects of that class\nThe Fmeasure of cluster i with respect to class j is F ( i j ) ( precision ( i j ) recall ( i j ) ) ( precision ( i j ) recall ( i j ) )\nPrecision recall and the Fmeasure can be calculated for each cluster\nThe precision is recall is and hence the F value is\nFor each class we take the maximum F measure attained for any cluster\nFor instance by deﬁnition a purity of is bad while a purity of is good at least if we trust our class labels and want our cluster structure to reﬂect the class structure\n<eos>"
    },
    {
        "key": "file2_topic_120",
        "label": "fmeasure, class, evaluate, fmeasures, tering",
        "input": "<task:clean> <sos>\nThe key idea of this approach is to evaluate whether a hierarchical clus tering contains for each class at least one cluster that is relatively pure and includes most of the objects of that class\nTo evaluate a hierarchical cluster ing with respect to this goal we compute for each class the Fmeasure for each cluster in the cluster hierarchy\nFinally we calculate an overall Fmeasure for the hierarchical clustering by computing the weighted average of all perclass Fmeasures where the weights are based on the class sizes\nMore formally Cluster Evaluation this hierarchical Fmeasure is deﬁned as follows m max F ( i j ) where the maximum is taken over all clusters i at all levels mj is the number of objects in class j and m is the total number of objects\n<eos>"
    }
]